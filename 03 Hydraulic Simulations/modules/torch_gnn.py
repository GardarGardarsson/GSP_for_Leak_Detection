#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Jul 31 11:49:15 2021

@author: gardar
"""

# --------------------------
# Importing public libraries
# --------------------------

# Operating system specific functions
import os


# PyTorch deep learning framework
import torch

import torch.nn.functional as F

from torch_geometric.nn import ChebConv


# Import Pandas for data handling
import pandas as pd

# Import numpy for array handling
import numpy as np

# --------------------------
# Importing custom libraries
# --------------------------

# To make sure we don't raise an error on importing project specific 
# libraries, we retrieve the path of the program file ...
filepath = os.path.dirname(os.path.realpath(__file__))

# ... set that as our working directory ...
os.chdir(filepath)

# ... and hop back one level!
os.chdir('..')

# PyTorch early stopping callback
from utils.early_stopping import EarlyStopping

# Metrics
from utils.metrics import Metrics


class GNNbase(torch.nn.Module):
    
    def __init__(self, name, data_generator, device):
        super(GNNbase, self).__init__()
        self.name = name
        self.total_loss = 0
        self.data_generator = data_generator
        self.device = device
        self.model = None
        self.loss = 0
        self.rel_err = 0
        self.rel_err_obs = 0
        self.rel_err_hid = 0
        
    def train_one_epoch(self, optimizer):
        self.model.train()
        for batch in self.generator:
            batch = batch.to(self.device)
            optimizer.zero_grad()
            out = self.model(batch)
            loss = torch.nn.functional.mse_loss/(out, batch.y)
            loss.backward()
            optimizer.step()
            self.total_loss += loss.item() * batch.num_graphs
        return (self.total_loss) / len(self.data_generator.dataset)
    
    def validate(self):
        self.model.eval()
        n = len(self.data_generator.dataset)
        tot_loss = 0
        tot_rel_err = 0
        tot_rel_err_obs = 0
        tot_rel_err_hid = 0
        for batch in self.data_generator:
            batch = batch.to(self.device)
            out = self.model(batch)
            loss = torch.nn.functional.mse_loss(out, batch.y)
            rel_err = metrics.rel_err(out, batch.y)
            rel_err_obs = metrics.rel_err(out, 
                                          batch.y, 
                                          batch.x[:, -1].type(torch.bool))
            rel_err_hid = metrics.rel_err(out, 
                                          batch.y,
                                          ~batch.x[:, -1].type(torch.bool))
            self.tot_loss += loss.item() * batch.num_graphs
            self.tot_rel_err += rel_err.item() * batch.num_graphs
            self.tot_rel_err_obs += rel_err_obs.item() * batch.num_graphs
            self.tot_rel_err_hid += rel_err_hid.item() * batch.num_graphs
        self.loss = tot_loss/n
        self.rel_err = tot_rel_err/n
        self.rel_err_obs = tot_rel_err_obs / n
        self.rel_err_hid = tot_rel_err_hid / n

class ChebNet(GNNbase):
    def __init__(self, name, data_generator, device, in_channels, out_channels):
        super(ChebNet, self).__init__(name, data_generator, device)
        self.conv1 = ChebConv(in_channels, 120, K=240)
        self.conv2 = ChebConv(120, 60, K=120)
        self.conv3 = ChebConv(60, 30, K=20)
        self.conv4 = ChebConv(30, out_channels, K=1, bias=False)
        torch.nn.init.xavier_normal_(self.conv1.weight)
        torch.nn.init.zeros_(self.conv1.bias)
        torch.nn.init.xavier_normal_(self.conv2.weight)
        torch.nn.init.zeros_(self.conv2.bias)
        torch.nn.init.xavier_normal_(self.conv3.weight)
        torch.nn.init.zeros_(self.conv3.bias)
        torch.nn.init.xavier_normal_(self.conv4.weight)
    def forward(self, data):
        x, edge_index, edge_weight  = data.x, data.edge_index, data.weight
        x = F.silu(self.conv1(x, edge_index, edge_weight))
        x = F.silu(self.conv2(x, edge_index, edge_weight))
        x = F.silu(self.conv3(x, edge_index, edge_weight))
        x = self.conv4(x, edge_index, edge_weight)
        return torch.sigmoid(x)

# %%

# An object oriented library for handling EPANET files in Python
import epynet 

# Import a custom tool for converting EPANET .inp files to networkx graphs
from utils.epanet_loader import get_nx_graph

# SCADA timeseries dataloader
from utils.data_loader import dataGenerator

# Set the path to the EPANET input file
path_to_wdn = './water_networks/anytown.inp'

# Import the .inp file using the EPYNET library
wdn = epynet.Network(path_to_wdn)

# Solve hydraulic model for a single timestep
wdn.solve()

# Convert the file using a custom function, based on:
# https://github.com/BME-SmartLab/GraphConvWat 
G , pos , head = get_nx_graph(wdn, weight_mode='inv_pipe_length', get_head=True)

# Load the training timeseries data 
# This was generated by running the 'generate_dta.py' script for
# the 'anytown.inp' of the GraphConvWat project of G. Hajgat√≥ et al.
x_trn = np.load('./water_networks/anytown_data/trn_x.npy')
y_trn = np.load('./water_networks/anytown_data/trn_y.npy')
x_val = np.load('./water_networks/anytown_data/vld_x.npy')
y_val = np.load('./water_networks/anytown_data/vld_y.npy')

# ----------------
# Hyper-parameters
# ----------------
batch_size    = 40
learning_rate = 3e-4
decay         = 6e-6
shuffle       = False
epochs        = 100

# --------------
# Training setup
# --------------
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
   

trn_gnrtr = dataGenerator(G, x_trn, y_trn, batch_size, shuffle)
val_gnrtr = dataGenerator(G, x_val, y_val, batch_size, shuffle)

model = ChebNet(name           = 'ChebNet',
                data_generator = trn_gnrtr,
                device         = device, 
                in_channels    = np.shape(x_trn)[-1], 
                out_channels   = np.shape(y_trn)[-1]).to(device)


optimizer = torch.optim.Adam([dict(params=model.conv1.parameters(), weight_decay=decay),
                              dict(params=model.conv2.parameters(), weight_decay=decay),
                              dict(params=model.conv3.parameters(), weight_decay=decay),
                              dict(params=model.conv4.parameters(), weight_decay=0)],
                              lr  = learning_rate,
                              eps = 1e-7)

estop   = EarlyStopping(min_delta=.00001, patience=30)
            
# %%  

# ----------------
# Hyper-parameters
# ----------------
batch_size    = 40
learning_rate = 3e-4
decay         = 6e-6
shuffle       = False
epochs        = 100

# Instantiate the data generators
trn_gnrtr = dataGenerator(G, x_trn, y_trn, batch_size, shuffle)
val_gnrtr = dataGenerator(G, x_val, y_val, batch_size, shuffle)


model = ChebNet(np.shape(x_trn)[-1], np.shape(y_trn)[-1]).to(device)

optimizer = torch.optim.Adam([dict(params=model.conv1.parameters(), weight_decay=decay),
                              dict(params=model.conv2.parameters(), weight_decay=decay),
                              dict(params=model.conv3.parameters(), weight_decay=decay),
                              dict(params=model.conv4.parameters(), weight_decay=0)],
                              lr  = learning_rate,
                              eps = 1e-7)
estop   = EarlyStopping(min_delta=.00001, patience=30)

if args.scaling == 'standard':
    scale_y = np.std(y_trn)
    bias_y  = np.mean(y_trn)
    
metrics = Metrics(bias_y, scale_y, device)
best_val_loss   = np.inf
results = pd.DataFrame(columns=[
'trn_loss', 'vld_loss', 'vld_rel_err', 'vld_rel_err_o', 'vld_rel_err_h'
])
header  = ''.join(['{:^15}'.format(colname) for colname in results.columns])
header  = '{:^5}'.format('epoch') + header
for epoch in range(0, epochs):
    trn_loss = train_one_epoch()
    val_loss, val_rel_err, val_rel_err_obs, val_rel_err_hid = eval_metrics(val_gnrtr)
    new_results = pd.Series({
        'trn_loss'      : trn_loss,
        'val_loss'      : val_loss,
        'val_rel_err'   : val_rel_err,
        'val_rel_err_o' : val_rel_err_obs,
        'val_rel_err_h' : val_rel_err_hid
        })
    results = results.append(new_results, ignore_index=True)
    if epoch % 20 == 0:
        print(header)
    values  = ''.join(['{:^15.6f}'.format(value) for value in new_results.values])
    print('{:^5}'.format(epoch) + values)
    if val_loss < best_val_loss:
        best_val_loss   = val_loss
        torch.save(model.state_dict(), './models/best_model.pt')
    if estop.step(torch.tensor(val_loss)):
        print('Early stopping...')
        break