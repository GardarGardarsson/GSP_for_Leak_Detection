{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86acfd25",
   "metadata": {},
   "source": [
    "# Adapt Training Data for Prediction Task\n",
    "\n",
    "We have previously been training the model for reconstruction, i.e:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathcal{G_y}}(t)=\\mathcal{f}(~\\mathcal{G_x(t)}~)\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\hat{\\mathcal{G_y}}(t)$ is the complete graph signal at timestep $t$, $\\mathcal{G_x(t)}$ is the partial graph signal and $f$ is the learned function for the reconstruction.\n",
    "\n",
    "For succesful model invalidation however, we need to predict the complete graph signal at $t+1$, and for this we may require an arbitrary amount of previous timesteps to learn the optimal function $f$ for the prediction:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathcal{G_y}}(t+1)=\\mathcal{f}(~\\mathcal{G_x(t)~,~G_x(t-1)~,~G_x(t-2)~...~G_x(t-n)}~)\n",
    "\\end{equation}\n",
    "\n",
    "Now this means our datahandler must be able to generate the training data so that a single training sample $i$ consists of:\n",
    "\n",
    "\\begin{equation}\n",
    "x_i = \\mathcal{G_x(t)~,~G_x(t-1)~,~G_x(t-2)~...~G_x(t-n)} \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "y_i = \\hat{\\mathcal{G_y}}(t+1)\n",
    "\\end{equation}\n",
    "\n",
    "Let's make it happen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46caae",
   "metadata": {},
   "source": [
    "## Load the Nominal Model Pressure Data\n",
    "\n",
    "This is the simulation data from the BattLeDIM competition. <br>\n",
    "We load it from a `.csv` to a `Pandas DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875a0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch_geometric.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41e64ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = 'data/l-town-data/'\n",
    "nominal_pressure = pd.read_csv(path_to_data + 'nominal_pressure.csv', index_col = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5044a057",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>n3</th>\n",
       "      <th>n4</th>\n",
       "      <th>n5</th>\n",
       "      <th>n6</th>\n",
       "      <th>n7</th>\n",
       "      <th>n8</th>\n",
       "      <th>n9</th>\n",
       "      <th>n10</th>\n",
       "      <th>...</th>\n",
       "      <th>n773</th>\n",
       "      <th>n774</th>\n",
       "      <th>n775</th>\n",
       "      <th>n776</th>\n",
       "      <th>n777</th>\n",
       "      <th>n778</th>\n",
       "      <th>n779</th>\n",
       "      <th>n780</th>\n",
       "      <th>n781</th>\n",
       "      <th>n782</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.885649</td>\n",
       "      <td>28.229593</td>\n",
       "      <td>28.925112</td>\n",
       "      <td>33.828160</td>\n",
       "      <td>36.537888</td>\n",
       "      <td>31.185562</td>\n",
       "      <td>26.183756</td>\n",
       "      <td>37.625713</td>\n",
       "      <td>32.829617</td>\n",
       "      <td>27.756403</td>\n",
       "      <td>...</td>\n",
       "      <td>52.457670</td>\n",
       "      <td>50.842834</td>\n",
       "      <td>51.985100</td>\n",
       "      <td>45.279144</td>\n",
       "      <td>48.613106</td>\n",
       "      <td>46.187430</td>\n",
       "      <td>46.464058</td>\n",
       "      <td>47.713550</td>\n",
       "      <td>49.513973</td>\n",
       "      <td>49.027523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>28.900486</td>\n",
       "      <td>28.244280</td>\n",
       "      <td>28.939800</td>\n",
       "      <td>33.842846</td>\n",
       "      <td>36.552296</td>\n",
       "      <td>31.200220</td>\n",
       "      <td>26.198536</td>\n",
       "      <td>37.640057</td>\n",
       "      <td>32.844303</td>\n",
       "      <td>27.771090</td>\n",
       "      <td>...</td>\n",
       "      <td>52.476357</td>\n",
       "      <td>50.860176</td>\n",
       "      <td>52.003483</td>\n",
       "      <td>45.305267</td>\n",
       "      <td>48.638573</td>\n",
       "      <td>46.213013</td>\n",
       "      <td>46.489710</td>\n",
       "      <td>47.739033</td>\n",
       "      <td>49.539490</td>\n",
       "      <td>49.053160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>28.915424</td>\n",
       "      <td>28.259079</td>\n",
       "      <td>28.954600</td>\n",
       "      <td>33.857647</td>\n",
       "      <td>36.566822</td>\n",
       "      <td>31.214993</td>\n",
       "      <td>26.213410</td>\n",
       "      <td>37.654522</td>\n",
       "      <td>32.859100</td>\n",
       "      <td>27.785880</td>\n",
       "      <td>...</td>\n",
       "      <td>52.494904</td>\n",
       "      <td>50.877390</td>\n",
       "      <td>52.021740</td>\n",
       "      <td>45.331276</td>\n",
       "      <td>48.663930</td>\n",
       "      <td>46.238487</td>\n",
       "      <td>46.515255</td>\n",
       "      <td>47.764404</td>\n",
       "      <td>49.564884</td>\n",
       "      <td>49.078682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>28.930391</td>\n",
       "      <td>28.273897</td>\n",
       "      <td>28.969416</td>\n",
       "      <td>33.872475</td>\n",
       "      <td>36.581398</td>\n",
       "      <td>31.229792</td>\n",
       "      <td>26.228312</td>\n",
       "      <td>37.669020</td>\n",
       "      <td>32.873928</td>\n",
       "      <td>27.800697</td>\n",
       "      <td>...</td>\n",
       "      <td>52.512928</td>\n",
       "      <td>50.894110</td>\n",
       "      <td>52.039470</td>\n",
       "      <td>45.356632</td>\n",
       "      <td>48.688644</td>\n",
       "      <td>46.263317</td>\n",
       "      <td>46.540150</td>\n",
       "      <td>47.789130</td>\n",
       "      <td>49.589645</td>\n",
       "      <td>49.103565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>28.945330</td>\n",
       "      <td>28.288706</td>\n",
       "      <td>28.984215</td>\n",
       "      <td>33.887300</td>\n",
       "      <td>36.595974</td>\n",
       "      <td>31.244581</td>\n",
       "      <td>26.243195</td>\n",
       "      <td>37.683544</td>\n",
       "      <td>32.888744</td>\n",
       "      <td>27.815506</td>\n",
       "      <td>...</td>\n",
       "      <td>52.530228</td>\n",
       "      <td>50.910160</td>\n",
       "      <td>52.056496</td>\n",
       "      <td>45.381070</td>\n",
       "      <td>48.712467</td>\n",
       "      <td>46.287260</td>\n",
       "      <td>46.564156</td>\n",
       "      <td>47.812970</td>\n",
       "      <td>49.613506</td>\n",
       "      <td>49.127550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603600</th>\n",
       "      <td>28.254526</td>\n",
       "      <td>27.599018</td>\n",
       "      <td>28.294537</td>\n",
       "      <td>33.197620</td>\n",
       "      <td>35.908325</td>\n",
       "      <td>30.555126</td>\n",
       "      <td>25.552855</td>\n",
       "      <td>36.996414</td>\n",
       "      <td>32.199078</td>\n",
       "      <td>27.125828</td>\n",
       "      <td>...</td>\n",
       "      <td>52.390083</td>\n",
       "      <td>50.780120</td>\n",
       "      <td>51.918594</td>\n",
       "      <td>45.184193</td>\n",
       "      <td>48.520725</td>\n",
       "      <td>46.094650</td>\n",
       "      <td>46.371037</td>\n",
       "      <td>47.621143</td>\n",
       "      <td>49.421486</td>\n",
       "      <td>48.934597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603900</th>\n",
       "      <td>28.268282</td>\n",
       "      <td>27.612644</td>\n",
       "      <td>28.308163</td>\n",
       "      <td>33.211240</td>\n",
       "      <td>35.921730</td>\n",
       "      <td>30.568724</td>\n",
       "      <td>25.566566</td>\n",
       "      <td>37.009750</td>\n",
       "      <td>32.212696</td>\n",
       "      <td>27.139463</td>\n",
       "      <td>...</td>\n",
       "      <td>52.404682</td>\n",
       "      <td>50.793670</td>\n",
       "      <td>51.932964</td>\n",
       "      <td>45.204330</td>\n",
       "      <td>48.540360</td>\n",
       "      <td>46.114370</td>\n",
       "      <td>46.390812</td>\n",
       "      <td>47.640790</td>\n",
       "      <td>49.441154</td>\n",
       "      <td>48.954360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604200</th>\n",
       "      <td>28.282366</td>\n",
       "      <td>27.626598</td>\n",
       "      <td>28.322117</td>\n",
       "      <td>33.225180</td>\n",
       "      <td>35.935430</td>\n",
       "      <td>30.582640</td>\n",
       "      <td>25.580593</td>\n",
       "      <td>37.023396</td>\n",
       "      <td>32.226640</td>\n",
       "      <td>27.153416</td>\n",
       "      <td>...</td>\n",
       "      <td>52.421234</td>\n",
       "      <td>50.809032</td>\n",
       "      <td>51.949250</td>\n",
       "      <td>45.227543</td>\n",
       "      <td>48.562996</td>\n",
       "      <td>46.137093</td>\n",
       "      <td>46.413580</td>\n",
       "      <td>47.663430</td>\n",
       "      <td>49.463800</td>\n",
       "      <td>48.977104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604500</th>\n",
       "      <td>28.296803</td>\n",
       "      <td>27.640894</td>\n",
       "      <td>28.336412</td>\n",
       "      <td>33.239470</td>\n",
       "      <td>35.949455</td>\n",
       "      <td>30.596899</td>\n",
       "      <td>25.594973</td>\n",
       "      <td>37.037357</td>\n",
       "      <td>32.240917</td>\n",
       "      <td>27.167704</td>\n",
       "      <td>...</td>\n",
       "      <td>52.438650</td>\n",
       "      <td>50.825188</td>\n",
       "      <td>51.966385</td>\n",
       "      <td>45.251743</td>\n",
       "      <td>48.586594</td>\n",
       "      <td>46.160793</td>\n",
       "      <td>46.437347</td>\n",
       "      <td>47.687035</td>\n",
       "      <td>49.487434</td>\n",
       "      <td>49.000850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604800</th>\n",
       "      <td>28.311518</td>\n",
       "      <td>27.655470</td>\n",
       "      <td>28.350979</td>\n",
       "      <td>33.254030</td>\n",
       "      <td>35.963753</td>\n",
       "      <td>30.611430</td>\n",
       "      <td>25.609623</td>\n",
       "      <td>37.051582</td>\n",
       "      <td>32.255480</td>\n",
       "      <td>27.182270</td>\n",
       "      <td>...</td>\n",
       "      <td>52.456990</td>\n",
       "      <td>50.842213</td>\n",
       "      <td>51.984432</td>\n",
       "      <td>45.277313</td>\n",
       "      <td>48.611523</td>\n",
       "      <td>46.185840</td>\n",
       "      <td>46.462460</td>\n",
       "      <td>47.711980</td>\n",
       "      <td>49.512405</td>\n",
       "      <td>49.025936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2017 rows Ã— 782 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               n1         n2         n3         n4         n5         n6  \\\n",
       "0       28.885649  28.229593  28.925112  33.828160  36.537888  31.185562   \n",
       "300     28.900486  28.244280  28.939800  33.842846  36.552296  31.200220   \n",
       "600     28.915424  28.259079  28.954600  33.857647  36.566822  31.214993   \n",
       "900     28.930391  28.273897  28.969416  33.872475  36.581398  31.229792   \n",
       "1200    28.945330  28.288706  28.984215  33.887300  36.595974  31.244581   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "603600  28.254526  27.599018  28.294537  33.197620  35.908325  30.555126   \n",
       "603900  28.268282  27.612644  28.308163  33.211240  35.921730  30.568724   \n",
       "604200  28.282366  27.626598  28.322117  33.225180  35.935430  30.582640   \n",
       "604500  28.296803  27.640894  28.336412  33.239470  35.949455  30.596899   \n",
       "604800  28.311518  27.655470  28.350979  33.254030  35.963753  30.611430   \n",
       "\n",
       "               n7         n8         n9        n10  ...       n773       n774  \\\n",
       "0       26.183756  37.625713  32.829617  27.756403  ...  52.457670  50.842834   \n",
       "300     26.198536  37.640057  32.844303  27.771090  ...  52.476357  50.860176   \n",
       "600     26.213410  37.654522  32.859100  27.785880  ...  52.494904  50.877390   \n",
       "900     26.228312  37.669020  32.873928  27.800697  ...  52.512928  50.894110   \n",
       "1200    26.243195  37.683544  32.888744  27.815506  ...  52.530228  50.910160   \n",
       "...           ...        ...        ...        ...  ...        ...        ...   \n",
       "603600  25.552855  36.996414  32.199078  27.125828  ...  52.390083  50.780120   \n",
       "603900  25.566566  37.009750  32.212696  27.139463  ...  52.404682  50.793670   \n",
       "604200  25.580593  37.023396  32.226640  27.153416  ...  52.421234  50.809032   \n",
       "604500  25.594973  37.037357  32.240917  27.167704  ...  52.438650  50.825188   \n",
       "604800  25.609623  37.051582  32.255480  27.182270  ...  52.456990  50.842213   \n",
       "\n",
       "             n775       n776       n777       n778       n779       n780  \\\n",
       "0       51.985100  45.279144  48.613106  46.187430  46.464058  47.713550   \n",
       "300     52.003483  45.305267  48.638573  46.213013  46.489710  47.739033   \n",
       "600     52.021740  45.331276  48.663930  46.238487  46.515255  47.764404   \n",
       "900     52.039470  45.356632  48.688644  46.263317  46.540150  47.789130   \n",
       "1200    52.056496  45.381070  48.712467  46.287260  46.564156  47.812970   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "603600  51.918594  45.184193  48.520725  46.094650  46.371037  47.621143   \n",
       "603900  51.932964  45.204330  48.540360  46.114370  46.390812  47.640790   \n",
       "604200  51.949250  45.227543  48.562996  46.137093  46.413580  47.663430   \n",
       "604500  51.966385  45.251743  48.586594  46.160793  46.437347  47.687035   \n",
       "604800  51.984432  45.277313  48.611523  46.185840  46.462460  47.711980   \n",
       "\n",
       "             n781       n782  \n",
       "0       49.513973  49.027523  \n",
       "300     49.539490  49.053160  \n",
       "600     49.564884  49.078682  \n",
       "900     49.589645  49.103565  \n",
       "1200    49.613506  49.127550  \n",
       "...           ...        ...  \n",
       "603600  49.421486  48.934597  \n",
       "603900  49.441154  48.954360  \n",
       "604200  49.463800  48.977104  \n",
       "604500  49.487434  49.000850  \n",
       "604800  49.512405  49.025936  \n",
       "\n",
       "[2017 rows x 782 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nominal_pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df825c",
   "metadata": {},
   "source": [
    "## Load the Sensor Locations\n",
    "\n",
    "We need to get the node names that are equipped with pressure sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82432aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset configuration file\n",
    "with open(path_to_data + 'dataset_configuration.yml') as file:\n",
    "\n",
    "    # Load the configuration to a dictionary\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader) \n",
    "\n",
    "# Generate a list of integers, indicating the number of the node\n",
    "# at which a  pressure sensor is present\n",
    "sensors = [int(string.replace(\"n\", \"\")) for string in config['pressure_sensors']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff92e6",
   "metadata": {},
   "source": [
    "## `dataCleaner( )` Function\n",
    "\n",
    "This is the function we need to adapt for the new prediction learning task!<br>\n",
    "We should probably add a parameter to it, `n_timesteps` which indicates the number of timesteps that should be contained in the training sample $x_i$ for a given target $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d443419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the nominal pressure dataframe \n",
    "def dataCleaner(pressure_df, observed_nodes, \n",
    "                rescale=None, mode='sensor_mask', task='reconstruction', n_timesteps=None):\n",
    "    '''\n",
    "    Function for cleaning the pressure dataframes obtained by simulation of the\n",
    "    nominal system model supplied with the BattLeDIM competition.\n",
    "    The output format is suitable for ingestion by the GNN model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pressure_df : pd.DataFrame\n",
    "        Pandas dataframe where: \n",
    "            columns (x) = nodes\n",
    "            index   (y) = observations\n",
    "    sensor_list : list of ints\n",
    "        A list of numerical values indicating the sensors nodal placement..\n",
    "    scaling : str\n",
    "        'standard' - standard scaling\n",
    "        'minmax'   - min/max scaling\n",
    "    mode : str\n",
    "        'sensor_mask' - A per timestep stacked feature output np.array as per below\n",
    "        'n_timesteps' - A t-n timestep stacked feature output np.array as per below\n",
    "    task : str\n",
    "        'reconstruction' - Returns y[t]   for x[t],x[t-1]...x[t-n] timesteps\n",
    "        'prediction'     - Returns y[t+1] for x[t],x[t-1]...x[t-n] timesteps\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    if mode='sensor_mask'\n",
    "    \n",
    "    x : np.array(n_obs,n_nodes,2)\n",
    "        The incomplete pressure signal matrix w/ 'n' number of observations.\n",
    "        This is the feature vector (x) for the GNN model\n",
    "        \n",
    "        x =\n",
    "        [21.57, 1    <- n1, pressure at node 1 is observed\n",
    "         0.0  , 0    <- n2, pressure at node 2 is unknown\n",
    "         0.0  , 0    <- n3, ... unknown\n",
    "         22.43, 1    <- n4, ... observed\n",
    "         0.0  , 0    <- n5, ... unknown\n",
    "         ...     ]   etc.\n",
    "         \n",
    "    if mode='n_timesteps'\n",
    "    \n",
    "    x : np.array(n_obs,n_nodes,n_timesteps)\n",
    "        The incomplete pressure signal matrix w/ 'n' number of observations, for n timesteps\n",
    "        This is the feature vector (x) for the GNN model\n",
    "        \n",
    "        x =\n",
    "        [21.57, 22.81, 23.13, ... , t-n    <- n1, pressure at node 1 is observed\n",
    "         0.0  , 0.0  , 0.0  , ... , t-n    <- n2, pressure at node 2 is unknown\n",
    "         0.0  , 0.0  , 0.0  , ... , t-n    <- n3, ... unknown\n",
    "         22.43, 22.51, 23.41, ... , t-n    <- n4, ... observed\n",
    "         0.0  , 0.0  , 0.0  , ... , t-n    <- n5, ... unknown\n",
    "         ...     ]   etc.\n",
    "        \n",
    "    y : np.array(n_obs,n_nodes,2)\n",
    "        The complete pressure signal matrix w/ 'n' number of observations.\n",
    "        With this we may train the GNN in a supervised manner.\n",
    "        \n",
    "        y =\n",
    "        [21.57    <- n1, all values are observed\n",
    "         21.89    <- n2, \n",
    "         22.17    <- n3\n",
    "         22.43    <- n4\n",
    "         23.79    <- n5\n",
    "         ...  ]   etc.\n",
    "        \n",
    "    '''     \n",
    "    # The number of nodes in the passed dataframe\n",
    "    n_nodes = len(pressure_df.columns)\n",
    "    \n",
    "    # Rename the columns (n1, n2, ...) to numerical values (1, 2, ...)\n",
    "    pressure_df.columns = [number for number in range(1,n_nodes+1)]\n",
    "    \n",
    "    # Perform scaling on the initial Pandas Dataframe for brevity\n",
    "    # This is less trivial than applying it on the later generated numpy arrays\n",
    "    \n",
    "    # Standard scale:\n",
    "    if rescale == 'standard':\n",
    "        _avg        = pressure_df.stack().mean()        # Calc. avg. over entire df.\n",
    "        _std        = pressure_df.stack().std(ddof=0)   # Calc. std.. over entire df.\n",
    "        bias        = _avg                              # Avg. is the scaling bias\n",
    "        scale       = _std                              # Std.dev. is the scaling range\n",
    "        pressure_df = (pressure_df - bias) / scale      # Scale to range\n",
    "        \n",
    "    # Min/max scaling (normalising):\n",
    "    elif rescale == 'minmax':\n",
    "        _min        = min(pressure_df.min())            # Find the absolute minimum value \n",
    "        _max        = max(pressure_df.max())            # Find the absolute maximum value\n",
    "        _rng        = _max - _min                       # Calculate the difference between (range)\n",
    "        bias        = _min                              # Scaling bias is the min value\n",
    "        scale       = _rng                              # Scaling range is the min-max range\n",
    "        pressure_df = (pressure_df - bias) / scale      # Scale to range\n",
    "        \n",
    "    # Perform no scaling\n",
    "    else:\n",
    "        bias        = None\n",
    "        scale       = None\n",
    "    \n",
    "    # DataFrame where the index is the node number holding the sensor and the value is set to 1\n",
    "    sensor_df = pd.DataFrame(data=[1 for i in observed_nodes],index=observed_nodes)\n",
    "    \n",
    "    # Filled single row of DataFrame with the complete number of nodes, the unmonitored nodes are set to 0 \n",
    "    sensor_df = sensor_df.reindex(list(range(1,n_nodes+1)),fill_value=0)\n",
    "    \n",
    "    # Find the number of rows in the DataFrame to be masked...\n",
    "    n_rows = len(pressure_df)\n",
    "    \n",
    "    # ... and complete a mask DataFrame, where all the observations to keep are set to 1 and the rest to 0\n",
    "    mask_df = sensor_df.T.append([sensor_df.T for i in range(n_rows-1)],ignore_index=True)\n",
    "    \n",
    "    # Enforce matching indices of the two DataFrames to be broadcast together\n",
    "    mask_df.index = pressure_df.index\n",
    "    \n",
    "    # Returns a (n_observations, n_nodes, 2) feature vector (x) where the 3rd dimension is a 0/1 mask \n",
    "    # of the observed nodes\n",
    "    if mode=='sensor_mask':\n",
    "        \n",
    "        # Generating the incomplete feature matrix (x)\n",
    "        x_mask = np.array(mask_df)\n",
    "        x_arr  = np.array(pressure_df.where(cond=mask_df==1,other = 0.0))\n",
    "        x      = np.stack((x_arr,x_mask),axis=2)\n",
    "\n",
    "        # Generating the complete label matrix (y)\n",
    "        y_arr  = np.array(pressure_df)\n",
    "        y      = np.stack((y_arr, ),axis=2)\n",
    "    \n",
    "    # Returns a (n_observations, n_nodes, n_timesteps) feature vector (x) where the 3rd dimension\n",
    "    # is the timesteps t, t-1, t-2 ... t-n leading to the observation to be predicted, at t+1\n",
    "    if mode=='n_timesteps':\n",
    "        \n",
    "        x_df         = pressure_df.where(cond=mask_df==1,other = 0.0)   # The feature dataframe (missing observations)\n",
    "        y_df         = pressure_df                                      # The label dataframe (complete observation)\n",
    "        \n",
    "        if task == 'prediction':                                        # If we're doing prediction we set the\n",
    "            n_samples = len(x_df)                                       # no.of samples as length of DF\n",
    "            \n",
    "        elif task == 'reconstruction':                                  # If we're doing reconstruction we set the\n",
    "            n_samples = len(x_df)+1                                     # no.of samples as length of DF + 1 due to\n",
    "                                                                        # slicing\n",
    "                \n",
    "        window_start = 0                                                # Set the start/end of the rolling window\n",
    "        window_end   = n_timesteps                                      # to be used to retrieve t-n timesteps for x\n",
    "\n",
    "        x_ = []                                                         # Initialise temp x_ and y_ lists\n",
    "        y_ = []                                                         # to contain our features and vectors\n",
    "\n",
    "        for i in range(n_timesteps,n_samples):                          # For each training sample\n",
    "            x_arr = (x_df.iloc[window_start:window_end].to_numpy().T)   # Add the t-n partial pressure signals\n",
    "            x_.append(np.flip(x_arr,axis=1))                            # Flip the order so that t is at index 0\n",
    "                                                                        # t-1 is at index 1, and so on\n",
    "            \n",
    "            if task == 'prediction':                                    # For prediction\n",
    "                y_.append(y_df.iloc[i])                                 # Add complete observation at t+1 as label\n",
    "                \n",
    "            elif task == 'reconstruction':                              # For reconstruction\n",
    "                y_.append(y_df.iloc[i-1])                               # Add complete observation at t as label\n",
    "            \n",
    "            window_start+=1                                             # Increment the\n",
    "            window_end  +=1                                             # rolling window\n",
    "\n",
    "        x = np.array(x_)                                                # Dump our lists \n",
    "        y = np.array(y_)                                                # to arrays \n",
    "        \n",
    "        row,col = y.shape                                               # Reshape the label array y\n",
    "        shape   = (row,col,1)                                           # so its dimensions are (n_observations, 1)\n",
    "        y = y.reshape(shape)                                            # not (n_observations, )\n",
    "        \n",
    "    return x,y,scale,bias                                               # Return the features, labels, scale & bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b670168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,scale,bias = dataCleaner(pressure_df    = nominal_pressure, \n",
    "                             observed_nodes = sensors,\n",
    "                             rescale        = 'minmax',\n",
    "                             mode           = 'n_timesteps',\n",
    "                             task           = 'prediction',\n",
    "                             n_timesteps    = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "329744eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = [1,1,1,2,3,6,12,24]\n",
    "modes     = ['n_timesteps' for i in range(len(timesteps)-1)]\n",
    "modes.insert(0,'sensor_mask')\n",
    "tasks     = ['prediction' for i in range(len(timesteps)-2)]\n",
    "tasks.insert(0,'reconstruction')\n",
    "tasks.insert(0,'reconstruction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5935053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_task_name(task, mode, timestep):\n",
    "    message = (' '*5 + str(timestep)+' '*5+task.upper()+' '*5+mode.upper()+' '*5).replace('_',' ')\n",
    "    print('+' + '-' * len(message) + '+')\n",
    "    print('|' + ' ' * len(message) + '|')\n",
    "    print('|' +           message  + '|')\n",
    "    print('|' + ' ' * len(message) + '|')\n",
    "    print('+' + '-' * len(message) + '+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "324310c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|                                              |\n",
      "|     1     RECONSTRUCTION     SENSOR MASK     |\n",
      "|                                              |\n",
      "+----------------------------------------------+\n",
      "x-shape:\t(2017, 782, 2)\n",
      "y-shape:\t(2017, 782, 1)\n",
      "+----------------------------------------------+\n",
      "|                                              |\n",
      "|     1     RECONSTRUCTION     N TIMESTEPS     |\n",
      "|                                              |\n",
      "+----------------------------------------------+\n",
      "x-shape:\t(2017, 782, 1)\n",
      "y-shape:\t(2017, 782, 1)\n",
      "+------------------------------------------+\n",
      "|                                          |\n",
      "|     1     PREDICTION     N TIMESTEPS     |\n",
      "|                                          |\n",
      "+------------------------------------------+\n",
      "x-shape:\t(2016, 782, 1)\n",
      "y-shape:\t(2016, 782, 1)\n",
      "+------------------------------------------+\n",
      "|                                          |\n",
      "|     2     PREDICTION     N TIMESTEPS     |\n",
      "|                                          |\n",
      "+------------------------------------------+\n",
      "x-shape:\t(2015, 782, 2)\n",
      "y-shape:\t(2015, 782, 1)\n",
      "+------------------------------------------+\n",
      "|                                          |\n",
      "|     3     PREDICTION     N TIMESTEPS     |\n",
      "|                                          |\n",
      "+------------------------------------------+\n",
      "x-shape:\t(2014, 782, 3)\n",
      "y-shape:\t(2014, 782, 1)\n",
      "+------------------------------------------+\n",
      "|                                          |\n",
      "|     6     PREDICTION     N TIMESTEPS     |\n",
      "|                                          |\n",
      "+------------------------------------------+\n",
      "x-shape:\t(2011, 782, 6)\n",
      "y-shape:\t(2011, 782, 1)\n",
      "+-------------------------------------------+\n",
      "|                                           |\n",
      "|     12     PREDICTION     N TIMESTEPS     |\n",
      "|                                           |\n",
      "+-------------------------------------------+\n",
      "x-shape:\t(2005, 782, 12)\n",
      "y-shape:\t(2005, 782, 1)\n",
      "+-------------------------------------------+\n",
      "|                                           |\n",
      "|     24     PREDICTION     N TIMESTEPS     |\n",
      "|                                           |\n",
      "+-------------------------------------------+\n",
      "x-shape:\t(1993, 782, 24)\n",
      "y-shape:\t(1993, 782, 1)\n"
     ]
    }
   ],
   "source": [
    "for task, mode, timestep in zip(tasks,modes,timesteps):\n",
    "    print_task_name(task, mode, timestep)\n",
    "    \n",
    "    x,y,scale,bias = dataCleaner(pressure_df    = nominal_pressure, \n",
    "                             observed_nodes = sensors,\n",
    "                             rescale        = 'minmax',\n",
    "                             mode           = mode,\n",
    "                             task           = task,\n",
    "                             n_timesteps    = timestep)\n",
    "    \n",
    "    print('x-shape:\\t{}'.format(x.shape))\n",
    "    print('y-shape:\\t{}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f41903",
   "metadata": {},
   "source": [
    "## Sanity Check  `for`-Loop to Split the Training Data\n",
    "\n",
    "Lets see if we can make sense out of this. <br>\n",
    "We can define a window size `n` that indicates the number of timesteps for each target. <br>\n",
    "To slide the window over the training data, we create `first` and `last` indexes which we increment simultaneously. <br>\n",
    "`last` is initialised as `n` so at the first iteration for `n=3` we have `first=0` and `last=3`. <br>\n",
    "This means we can slice from `x` with `x[first:last]`, or `x[0:3]`. <br>\n",
    "This slice returns `x[0], x[1] and x[2]`. <br>\n",
    "Our `for`-loop will run from `n` to `len(y)` with `i` as the iterator. <br>\n",
    "Thus, the first target `y`, for the training samples `x[0:3]` can be retrieved as `y[i]` or `y[3]` in the first instance. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "185c88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = (0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "621a22e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2]), array([2, 3]), array([3, 4]), array([4, 5]), array([5, 6])]\n",
      "['c', 'd', 'e', 'f', 'g']\n"
     ]
    }
   ],
   "source": [
    "n     = 2   # The number of timesteps to contain in x\n",
    "first = 0   # The first index at the first timestep, we will increment this\n",
    "last  = n   # The last index at the first timestep, this will also be incremented\n",
    "x_p   = []  # An empty list to contain the training samples\n",
    "y_p   = []  # An empty list to contain the targets\n",
    "\n",
    "_y = ['a', 'b', 'c', 'd', 'e', 'f', 'g']  # Dummy lists for\n",
    "_x = [ 1 ,  2 ,  3 ,  4 ,  5 ,  6 ,  7 ]  # sanity checking\n",
    "\n",
    "for i in range(n,len(_y)):\n",
    "    x_p.append(np.array(_x[first:last]))  # Get a window of training samples\n",
    "    y_p.append(_y[i])                     # Get a single target for the given window\n",
    "    first += 1                            # Increment first window index\n",
    "    last  += 1                            # Increment second window index\n",
    "    \n",
    "print(x_p)\n",
    "print(y_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca248e",
   "metadata": {},
   "source": [
    "Let's create a function out of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60ab9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionTaskDataSplitter(x, y, n_timesteps):\n",
    "    window_start = 0\n",
    "    window_end = n_timesteps\n",
    "    n_samples = len(y)\n",
    "    x_new = []\n",
    "    y_new = []\n",
    "    \n",
    "    for i in range(n_timesteps, n_samples):\n",
    "        x_new.append( np.array( x[window_start:window_end] ) )\n",
    "        y_new.append( y[i] )\n",
    "        window_start += 1\n",
    "        window_end   += 1  \n",
    "        \n",
    "    return np.array(x_new) , np.array(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cf96fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_p, y_p = predictionTaskDataSplitter(x=x, \n",
    "                                      y=y, \n",
    "                                      n_timesteps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4afb0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.08999907, 0.08968394, 0.08936828, ..., 0.0834867 ,\n",
       "          0.08318296, 0.08288128],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.0903132 , 0.08999907, 0.08968394, ..., 0.08379103,\n",
       "          0.0834867 , 0.08318296],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.09062625, 0.0903132 , 0.08999907, ..., 0.0840948 ,\n",
       "          0.08379103, 0.0834867 ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.0903132 , 0.08999907, 0.08968394, ..., 0.08379103,\n",
       "          0.0834867 , 0.08318296],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.09062625, 0.0903132 , 0.08999907, ..., 0.0840948 ,\n",
       "          0.08379103, 0.0834867 ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.09093908, 0.09062625, 0.0903132 , ..., 0.08439833,\n",
       "          0.0840948 , 0.08379103],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.09062625, 0.0903132 , 0.08999907, ..., 0.0840948 ,\n",
       "          0.08379103, 0.0834867 ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.09093908, 0.09062625, 0.0903132 , ..., 0.08439833,\n",
       "          0.0840948 , 0.08379103],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.09125114, 0.09093908, 0.09062625, ..., 0.08470228,\n",
       "          0.08439833, 0.0840948 ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.06950444, 0.06923529, 0.06896426, ..., 0.06409549,\n",
       "          0.06386137, 0.06364177],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.06977471, 0.06950444, 0.06923529, ..., 0.06434348,\n",
       "          0.06409549, 0.06386137],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.0700484 , 0.06977471, 0.06950444, ..., 0.06460371,\n",
       "          0.06434348, 0.06409549],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.06977471, 0.06950444, 0.06923529, ..., 0.06434348,\n",
       "          0.06409549, 0.06386137],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.0700484 , 0.06977471, 0.06950444, ..., 0.06460371,\n",
       "          0.06434348, 0.06409549],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.07032811, 0.0700484 , 0.06977471, ..., 0.06487209,\n",
       "          0.06460371, 0.06434348],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.0700484 , 0.06977471, 0.06950444, ..., 0.06460371,\n",
       "          0.06434348, 0.06409549],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.07032811, 0.0700484 , 0.06977471, ..., 0.06487209,\n",
       "          0.06460371, 0.06434348],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]],\n",
       "\n",
       "        [[0.07061448, 0.07032811, 0.0700484 , ..., 0.06514468,\n",
       "          0.06487209, 0.06460371],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c652b3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 782, 24)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_p[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "894cc433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(782, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa39494a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1990, 3, 782, 24)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca658c4a",
   "metadata": {},
   "source": [
    "## BattLeDIM DataLoader\n",
    "\n",
    "We also need to update it for prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55d19f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the nominal pressure dataframe\n",
    "def dataCleaner(pressure_df, observed_nodes,\n",
    "                rescale=None, mode='sensor_mask', task='reconstruction', n_timesteps=None):\n",
    "    '''\n",
    "    Function for cleaning the pressure dataframes obtained by simulation of the\n",
    "    nominal system model supplied with the BattLeDIM competition.\n",
    "    The output format is suitable for ingestion by the GNN model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pressure_df : pd.DataFrame\n",
    "        Pandas dataframe where:\n",
    "            columns (x) = nodes\n",
    "            index   (y) = observations\n",
    "    sensor_list : list of ints\n",
    "        A list of numerical values indicating the sensors nodal placement..\n",
    "    scaling : str\n",
    "        'standard' - standard scaling\n",
    "        'minmax'   - min/max scaling\n",
    "    mode : str\n",
    "        'sensor_mask' - A per timestep stacked feature output np.array as per below\n",
    "        'n_timesteps' - A t-n timestep stacked feature output np.array as per below\n",
    "    task : str\n",
    "        'reconstruction' - Returns y[t]   for x[t],x[t-1]...x[t-n] timesteps\n",
    "        'prediction'     - Returns y[t+1] for x[t],x[t-1]...x[t-n] timesteps\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    if mode='sensor_mask'\n",
    "    \n",
    "    x : np.array(n_obs,n_nodes,2)\n",
    "        The incomplete pressure signal matrix w/ 'n' number of observations.\n",
    "        This is the feature vector (x) for the GNN model\n",
    "        \n",
    "        x =\n",
    "        [21.57, 1    <- n1, pressure at node 1 is observed\n",
    "         0.0  , 0    <- n2, pressure at node 2 is unknown\n",
    "         0.0  , 0    <- n3, ... unknown\n",
    "         22.43, 1    <- n4, ... observed\n",
    "         0.0  , 0    <- n5, ... unknown\n",
    "         ...     ]   etc.\n",
    "         \n",
    "    if mode='n_timesteps'\n",
    "    \n",
    "    x : np.array(n_obs,n_nodes,n_timesteps)\n",
    "        The incomplete pressure signal matrix w/ 'n' number of observations, for n timesteps\n",
    "        This is the feature vector (x) for the GNN model\n",
    "        \n",
    "        x =\n",
    "        [21.57, 22.81, 23.13, ... , t-n    <- n1, pressure at node 1 is observed\n",
    "         0.0  , 0.0  , 0.0  , ... , t-n    <- n2, pressure at node 2 is unknown\n",
    "         0.0  , 0.0  , 0.0  , ... , t-n    <- n3, ... unknown\n",
    "         22.43, 22.51, 23.41, ... , t-n    <- n4, ... observed\n",
    "         0.0  , 0.0  , 0.0  , ... , t-n    <- n5, ... unknown\n",
    "         ...     ]   etc.\n",
    "        \n",
    "    y : np.array(n_obs,n_nodes,2)\n",
    "        The complete pressure signal matrix w/ 'n' number of observations.\n",
    "        With this we may train the GNN in a supervised manner.\n",
    "        \n",
    "        y =\n",
    "        [21.57    <- n1, all values are observed\n",
    "         21.89    <- n2,\n",
    "         22.17    <- n3\n",
    "         22.43    <- n4\n",
    "         23.79    <- n5\n",
    "         ...  ]   etc.\n",
    "        \n",
    "    '''\n",
    "    # The number of nodes in the passed dataframe\n",
    "    n_nodes = len(pressure_df.columns)\n",
    "    \n",
    "    # Rename the columns (n1, n2, ...) to numerical values (1, 2, ...)\n",
    "    pressure_df.columns = [number for number in range(1,n_nodes+1)]\n",
    "    \n",
    "    # Perform scaling on the initial Pandas Dataframe for brevity\n",
    "    # This is less trivial than applying it on the later generated numpy arrays\n",
    "    \n",
    "    # Standard scale:\n",
    "    if rescale == 'standard':\n",
    "        _avg        = pressure_df.stack().mean()        # Calc. avg. over entire df.\n",
    "        _std        = pressure_df.stack().std(ddof=0)   # Calc. std.. over entire df.\n",
    "        bias        = _avg                              # Avg. is the scaling bias\n",
    "        scale       = _std                              # Std.dev. is the scaling range\n",
    "        pressure_df = (pressure_df - bias) / scale      # Scale to range\n",
    "        \n",
    "    # Min/max scaling (normalising):\n",
    "    elif rescale == 'minmax':\n",
    "        _min        = min(pressure_df.min())            # Find the absolute minimum value\n",
    "        _max        = max(pressure_df.max())            # Find the absolute maximum value\n",
    "        _rng        = _max - _min                       # Calculate the difference between (range)\n",
    "        bias        = _min                              # Scaling bias is the min value\n",
    "        scale       = _rng                              # Scaling range is the min-max range\n",
    "        pressure_df = (pressure_df - bias) / scale      # Scale to range\n",
    "        \n",
    "    # Perform no scaling\n",
    "    else:\n",
    "        bias        = None\n",
    "        scale       = None\n",
    "    \n",
    "    # DataFrame where the index is the node number holding the sensor and the value is set to 1\n",
    "    sensor_df = pd.DataFrame(data=[1 for i in observed_nodes],index=observed_nodes)\n",
    "    \n",
    "    # Filled single row of DataFrame with the complete number of nodes, the unmonitored nodes are set to 0\n",
    "    sensor_df = sensor_df.reindex(list(range(1,n_nodes+1)),fill_value=0)\n",
    "    \n",
    "    # Find the number of rows in the DataFrame to be masked...\n",
    "    n_rows = len(pressure_df)\n",
    "    \n",
    "    # ... and complete a mask DataFrame, where all the observations to keep are set to 1 and the rest to 0\n",
    "    mask_df = sensor_df.T.append([sensor_df.T for i in range(n_rows-1)],ignore_index=True)\n",
    "    \n",
    "    # Enforce matching indices of the two DataFrames to be broadcast together\n",
    "    mask_df.index = pressure_df.index\n",
    "    \n",
    "    # Returns a (n_observations, n_nodes, 2) feature vector (x) where the 3rd dimension is a 0/1 mask\n",
    "    # of the observed nodes\n",
    "    if mode=='sensor_mask':\n",
    "        \n",
    "        # Generating the incomplete feature matrix (x)\n",
    "        x_mask = np.array(mask_df)\n",
    "        x_arr  = np.array(pressure_df.where(cond=mask_df==1,other = 0.0))\n",
    "        x      = np.stack((x_arr,x_mask),axis=2)\n",
    "\n",
    "        # Generating the complete label matrix (y)\n",
    "        y_arr  = np.array(pressure_df)\n",
    "        y      = np.stack((y_arr, ),axis=2)\n",
    "    \n",
    "    # Returns a (n_observations, n_nodes, n_timesteps) feature vector (x) where the 3rd dimension\n",
    "    # is the timesteps t, t-1, t-2 ... t-n leading to the observation to be predicted, at t+1\n",
    "    if mode=='n_timesteps':\n",
    "        \n",
    "        x_df         = pressure_df.where(cond=mask_df==1,other = 0.0)   # The feature dataframe (missing observations)\n",
    "        y_df         = pressure_df                                      # The label dataframe (complete observation)\n",
    "        \n",
    "        if task == 'prediction':                                        # If we're doing prediction we set the\n",
    "            n_samples = len(x_df)                                       # no.of samples as length of DF\n",
    "            \n",
    "        elif task == 'reconstruction':                                  # If we're doing reconstruction we set the\n",
    "            n_samples = len(x_df)+1                                     # no.of samples as length of DF + 1 due to\n",
    "                                                                        # slicing\n",
    "                \n",
    "        window_start = 0                                                # Set the start/end of the rolling window\n",
    "        window_end   = n_timesteps                                      # to be used to retrieve t-n timesteps for x\n",
    "\n",
    "        x_ = []                                                         # Initialise temp x_ and y_ lists\n",
    "        y_ = []                                                         # to contain our features and vectors\n",
    "\n",
    "        for i in range(n_timesteps,n_samples):                          # For each training sample\n",
    "            x_arr = (x_df.iloc[window_start:window_end].to_numpy().T)   # Add the t-n partial pressure signals\n",
    "            x_.append(np.flip(x_arr,axis=1))                            # Flip the order so that t is at index 0\n",
    "                                                                        # t-1 is at index 1, and so on\n",
    "            \n",
    "            if task == 'prediction':                                    # For prediction\n",
    "                y_.append(y_df.iloc[i])                                 # Add complete observation at t+1 as label\n",
    "                \n",
    "            elif task == 'reconstruction':                              # For reconstruction\n",
    "                y_.append(y_df.iloc[i-1])                               # Add complete observation at t as label\n",
    "            \n",
    "            window_start+=1                                             # Increment the\n",
    "            window_end  +=1                                             # rolling window\n",
    "\n",
    "        x = np.array(x_)                                                # Dump our lists\n",
    "        y = np.array(y_)                                                # to arrays\n",
    "        \n",
    "        row,col = y.shape                                               # Reshape the label array y\n",
    "        shape   = (row,col,1)                                           # so its dimensions are (n_observations, 1)\n",
    "        y = y.reshape(shape)                                            # not (n_observations, )\n",
    "        \n",
    "    return x,y,scale,bias                                               # Return the features, labels, scale & bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f26b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a889e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a3d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a800397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the timeseries datasets of the BattLeDIM challenge\n",
    "def battledimLoader(observed_nodes, n_nodes=782, path='./BattLeDIM/', file='2018_SCADA_Pressures.csv', \n",
    "                    rescale=False, scale=None, bias=None,\n",
    "                    mode='sensor_mask',task='reconstruction',n_timesteps=None):\n",
    "    '''\n",
    "    Function for loading the SCADA .csv datasets of the BattLeDIM competition\n",
    "    and returning it in a dataformat suitable for the GNN model to ingest.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observed_nodes : list of ints\n",
    "        A list of numerical values indicating the sensors nodal placement.\n",
    "    n_nodes : int, optional\n",
    "        Total no. of nodes in the network. The default is 782.\n",
    "    path : str, optional\n",
    "        Directory name containing SCADA data. The default is './BattLeDIM/'.\n",
    "    file : str, optional\n",
    "        Filename. The default is '2018_SCADA_Pressures.csv'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : np.array(n_obs,n_nodes,2)\n",
    "        An array of size (n_observations x n_nodes x 2).\n",
    "        These are then n number of 2-d matrices where the 1st dimension is\n",
    "        nodal pressure value, and the 2nd dimension is a mask, 1 if the\n",
    "        pressure value is present (at the observed nodes) and 0 if not\n",
    "        \n",
    "        E.g.:\n",
    "        \n",
    "        [21.57, 1    <- n1, pressure at node 1 is observed\n",
    "         0.0  , 0    <- n2, pressure at node 2 is unknown\n",
    "         0.0  , 0    <- n3\n",
    "         22.43, 1    <- n4\n",
    "         0.0  , 0    <- n5\n",
    "         ...     ]   etc.\n",
    "    '''\n",
    "    # Read the file at the passed destination into a Pandas DataFrame\n",
    "    df = pd.read_csv(str(path + file), sep=';', decimal=',')\n",
    "    \n",
    "    # Set the 'Timestamp' column as the index\n",
    "    df = df.set_index('Timestamp')\n",
    "    \n",
    "    # Set the column names as the numeric list passed into the function\n",
    "    # which states what nodes of the graphs are observed\n",
    "    df.columns = observed_nodes\n",
    "    \n",
    "    # User has option of rescaling the imported data\n",
    "    if rescale:\n",
    "        df = (df - bias) / scale\n",
    "        \n",
    "    # Generate a temporary image of the DataFrame, that's been filled with zeros\n",
    "    # at the un-observed nodes\n",
    "    temp = df.T.reindex(list(range(1,n_nodes+1)),fill_value=0.0)\n",
    "        \n",
    "    if mode=='sensor_mask':\n",
    "\n",
    "        # Create a \"mask\" array, that's set to 1 at the observed nodes and 0 otherwise\n",
    "        arr2 = np.array(temp.mask(temp>0.0,1).astype('int'))\n",
    "\n",
    "        # Create a numpy array from the temporary image\n",
    "        arr1 = np.array(temp)\n",
    "\n",
    "        # Stack and transpose the observation and mask arrays\n",
    "        result = np.stack((arr1,arr2),axis=0).T\n",
    "        \n",
    "    # Returns a (n_observations, n_nodes, n_timesteps) feature vector (x) where the 3rd dimension\n",
    "    # is the timesteps t, t-1, t-2 ... t-n leading to the observation to be predicted, at t+1\n",
    "    if mode=='n_timesteps':\n",
    "        \n",
    "        x_df = temp.T                                                   # The feature dataframe (missing observations)\n",
    "\n",
    "        if task == 'prediction':                                        # If we're doing prediction we set the\n",
    "            n_samples = len(x_df)                                       # no.of samples as length of DF\n",
    "            \n",
    "        elif task == 'reconstruction':                                  # If we're doing reconstruction we set the\n",
    "            n_samples = len(x_df)+1                                     # no.of samples as length of DF + 1 due to\n",
    "                                                                        # slicing\n",
    "                \n",
    "        window_start = 0                                                # Set the start/end of the rolling window\n",
    "        window_end   = n_timesteps                                      # to be used to retrieve t-n timesteps for x\n",
    "\n",
    "        x_ = []                                                         # Initialise temp x_ and y_ lists\n",
    "                                                                        # to contain our features and vectors\n",
    "\n",
    "        for i in range(n_timesteps,n_samples):                          # For each training sample\n",
    "            x_arr = (x_df.iloc[window_start:window_end].to_numpy().T)   # Add the t-n partial pressure signals\n",
    "            x_.append(np.flip(x_arr,axis=1))                            # Flip the order so that t is at index 0\n",
    "            window_start+=1                                             # Increment the\n",
    "            window_end  +=1                                             # rolling window\n",
    "\n",
    "        x = np.array(x_)                                                # Dump our lists\n",
    "        \n",
    "        result = x\n",
    "    \n",
    "    # Return the results\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4b26e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a numpy array with format matching the GraphConvWat problem\n",
    "pressure_2018 = battledimLoader(observed_nodes = sensors,\n",
    "                                n_nodes        = 782,\n",
    "                                path           = path_to_data,\n",
    "                                file           = '2018_SCADA_Pressures.csv',\n",
    "                                rescale        = True, \n",
    "                                scale          = scale,\n",
    "                                bias           = bias,\n",
    "                                task           = 'prediction',\n",
    "                                mode           = 'n_timesteps',\n",
    "                                n_timesteps    = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86ac6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
