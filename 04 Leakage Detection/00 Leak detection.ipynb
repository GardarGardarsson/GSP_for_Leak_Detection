{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88fb90b",
   "metadata": {},
   "source": [
    "# Leak Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c9c5b",
   "metadata": {},
   "source": [
    "## 1. Make Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b838c",
   "metadata": {},
   "source": [
    "Convert the `EPANET` model to a `networkx` graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e944d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import epynet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.epanet_loader import get_nx_graph\n",
    "from utils.epanet_simulator import epanetSimulator\n",
    "from utils.data_loader import battledimLoader, dataCleaner, dataGenerator, embedSignalOnGraph, rescaleSignal\n",
    "from modules.torch_gnn import ChebNet\n",
    "from utils.visualisation import visualise\n",
    "\n",
    "# Runtime configuration\n",
    "path_to_wdn     = './data/L-TOWN.inp'\n",
    "path_to_data    = './data/l-town-data/'\n",
    "weight_mode     = 'pipe_length'\n",
    "self_loops      = True\n",
    "scaling         = 'minmax'\n",
    "figsize         = (50,16)\n",
    "print_out_rate  = 1               \n",
    "model_name      = 'l-town-chebnet-' + weight_mode +'-' + scaling + '{}'.format('-self_loop' if self_loops else '')\n",
    "last_model_path = './studies/models/' + model_name + '-1.pt'\n",
    "last_log_path   = './studies/logs/'   + model_name + '-1.csv' \n",
    "\n",
    "# Import the .inp file using the EPYNET library\n",
    "wdn = epynet.Network(path_to_wdn)\n",
    "\n",
    "# Solve hydraulic model for a single timestep\n",
    "wdn.solve()\n",
    "\n",
    "# Convert the file using a custom function, based on:\n",
    "# https://github.com/BME-SmartLab/GraphConvWat \n",
    "G , pos , head = get_nx_graph(wdn, weight_mode=weight_mode, get_head=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813e8928",
   "metadata": {},
   "source": [
    "Get a list of node IDs with sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddec7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset configuration file\n",
    "with open(path_to_data + 'dataset_configuration.yml') as file:\n",
    "\n",
    "    # Load the configuration to a dictionary\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader) \n",
    "\n",
    "# Generate a list of integers, indicating the number of the node\n",
    "# at which a  pressure sensor is present\n",
    "sensors = [int(string.replace(\"n\", \"\")) for string in config['pressure_sensors']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39199da",
   "metadata": {},
   "source": [
    "Add self-loops to the sensor nodes in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e3ceb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if self_loops:\n",
    "    for sensor_node in sensors:             # For each node in the sensor list\n",
    "        G.add_edge(u_of_edge=sensor_node,   # Add an edge from that node ...\n",
    "                   v_of_edge=sensor_node,   # ... to itself ...\n",
    "                   weight=1.,name='SELF')   # ... and set its weight to equal 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d40ab9",
   "metadata": {},
   "source": [
    "## 2. Get Simulation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1de321",
   "metadata": {},
   "source": [
    "We run an EPANET simulation using the WNTR library and the EPANET\n",
    "nominal model supplied with the BattLeDIM competition. <br>\n",
    "With this simulation, we have a complete pressure signal for all\n",
    "nodes in the network, on which the GNN algorithm is to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd89db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the nominal WDN model\n",
    "nominal_wdn_model = epanetSimulator(path_to_wdn, path_to_data)\n",
    "\n",
    "# Run a simulation\n",
    "nominal_wdn_model.simulate()\n",
    "\n",
    "# Retrieve the nodal pressures\n",
    "nominal_pressure = nominal_wdn_model.get_simulated_pressure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230405b",
   "metadata": {},
   "source": [
    "Populate feature vector x and label vector y from the nominal pressures. <br>\n",
    "Also retrieve the scale and bias of the scaling transformation. <br>\n",
    "This is so we can inverse transform the predicted values to calculate relative reconstruction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5376c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,scale,bias = dataCleaner(pressure_df    = nominal_pressure, # Pass the nodal pressures\n",
    "                             observed_nodes = sensors,          # Indicate which nodes have sensors\n",
    "                             rescale        = scaling)          # Perform scaling on the timeseries data\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, \n",
    "                                              test_size    = 0.2,\n",
    "                                              random_state = 1,\n",
    "                                              shuffle      = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88350332",
   "metadata": {},
   "source": [
    "## 3. Get Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf14ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a numpy array with format matching the GraphConvWat problem\n",
    "pressure_2018 = battledimLoader(observed_nodes = sensors,\n",
    "                                n_nodes        = 782,\n",
    "                                path           = path_to_data,\n",
    "                                file           = '2018_SCADA_Pressures.csv',\n",
    "                                rescale        = True, \n",
    "                                scale          = scale,\n",
    "                                bias           = bias)\n",
    "\n",
    "pressure_2019 = battledimLoader(observed_nodes = sensors,\n",
    "                                n_nodes        = 782,\n",
    "                                path           = path_to_data,\n",
    "                                file           = '2019_SCADA_Pressures.csv',\n",
    "                                rescale        = True, \n",
    "                                scale          = scale,\n",
    "                                bias           = bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820a46a",
   "metadata": {},
   "source": [
    "## 4. Load a Trained GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7784fe40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the computation device as NVIDIA GPU if available else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate a Chebysev Network GNN model\n",
    "model  = ChebNet(name           = 'ChebNet',\n",
    "                 data_generator = None,\n",
    "                 device         = device, \n",
    "                 in_channels    = np.shape(x_trn)[-1], \n",
    "                 out_channels   = np.shape(y_trn)[-1],\n",
    "                 data_scale     = scale, \n",
    "                 data_bias      = bias).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd49ed5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Loaded previous model results...\n",
      "                --------------------------------------------------\n",
      "                Model has been trained for:\t100 epochs\n",
      "                Best validation loss:      \t1.6016847697140293e-05 \n",
      "                Occurred in training round:\t96 \n"
     ]
    }
   ],
   "source": [
    "# We offer the user the option to load the previously trained weights\n",
    "model.load_model(last_model_path, last_log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce5b2b",
   "metadata": {},
   "source": [
    "## 5. Predict a Year's worth of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b683449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pressure(graph, pressure_series, print_out_rate=100, save=True, filename='predictions.csv'):\n",
    "    results = []\n",
    "    elapsed_time = time.time()\n",
    "    for i, partial_graph_signal in enumerate(pressure_series):\n",
    "        if not i % print_out_rate:\n",
    "            execution_time = time.time()\n",
    "        \n",
    "        results.append(model.predict(graph, partial_graph_signal))\n",
    "        \n",
    "        if not i % print_out_rate:\n",
    "            print('Signal:\\t{}\\t Execution:\\t{:.3f} s\\t Elapsed:\\t{:.3f} s'.format(i,\n",
    "                                                                                   time.time()-execution_time, \n",
    "                                                                                   time.time()-elapsed_time))\n",
    "    if save:\n",
    "        print('-'*63+\"\\nSaving results to: \\n{}/{}\\n\\n\".format(os.getcwd(),filename))\n",
    "        pd.DataFrame(results).to_csv(filename)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "018a9b97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal:\t0\t Execution:\t0.266 s\t Elapsed:\t0.266 s\n",
      "Signal:\t1\t Execution:\t0.262 s\t Elapsed:\t0.528 s\n",
      "Signal:\t2\t Execution:\t0.256 s\t Elapsed:\t0.785 s\n",
      "Signal:\t3\t Execution:\t0.251 s\t Elapsed:\t1.036 s\n",
      "Signal:\t4\t Execution:\t0.252 s\t Elapsed:\t1.288 s\n",
      "Signal:\t5\t Execution:\t0.257 s\t Elapsed:\t1.545 s\n",
      "Signal:\t6\t Execution:\t0.253 s\t Elapsed:\t1.799 s\n",
      "Signal:\t7\t Execution:\t0.258 s\t Elapsed:\t2.057 s\n",
      "Signal:\t8\t Execution:\t0.287 s\t Elapsed:\t2.343 s\n",
      "Signal:\t9\t Execution:\t0.289 s\t Elapsed:\t2.633 s\n",
      "Signal:\t10\t Execution:\t0.275 s\t Elapsed:\t2.908 s\n",
      "---------------------------------------------------------------\n",
      "Saving results to: \n",
      "/Users/gardar/Documents/UCL/ELEC0054 IMLS Research Project/04 Implementation/04 Leakage Detection/2018_predictions.csv\n",
      "\n",
      "\n",
      "Signal:\t0\t Execution:\t0.299 s\t Elapsed:\t0.299 s\n",
      "Signal:\t1\t Execution:\t0.310 s\t Elapsed:\t0.609 s\n",
      "Signal:\t2\t Execution:\t0.283 s\t Elapsed:\t0.892 s\n",
      "Signal:\t3\t Execution:\t0.278 s\t Elapsed:\t1.170 s\n",
      "Signal:\t4\t Execution:\t0.255 s\t Elapsed:\t1.425 s\n",
      "Signal:\t5\t Execution:\t0.247 s\t Elapsed:\t1.672 s\n",
      "Signal:\t6\t Execution:\t0.261 s\t Elapsed:\t1.933 s\n",
      "Signal:\t7\t Execution:\t0.244 s\t Elapsed:\t2.178 s\n",
      "Signal:\t8\t Execution:\t0.252 s\t Elapsed:\t2.429 s\n",
      "Signal:\t9\t Execution:\t0.247 s\t Elapsed:\t2.676 s\n",
      "Signal:\t10\t Execution:\t0.255 s\t Elapsed:\t2.932 s\n",
      "---------------------------------------------------------------\n",
      "Saving results to: \n",
      "/Users/gardar/Documents/UCL/ELEC0054 IMLS Research Project/04 Implementation/04 Leakage Detection/2019_predictions.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_2018 = predict_pressure(G, \n",
    "                                   pressure_2018[:11], \n",
    "                                   print_out_rate = print_out_rate, \n",
    "                                   save           = True, \n",
    "                                   filename       = '2018_predictions.csv')\n",
    "\n",
    "prediction_2019 = predict_pressure(G, \n",
    "                                   pressure_2019[:11], \n",
    "                                   print_out_rate = print_out_rate, \n",
    "                                   save           = True, \n",
    "                                   filename       = '2019_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd48d98",
   "metadata": {},
   "source": [
    "## 6. Read in Yearly Prediction and Scale Back to Original Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ae7bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2018_predictions.csv', index_col='Unnamed: 0')\n",
    "df.columns = ['n{}'.format(int(node)+1) for node in df.columns]\n",
    "df = df*scale+bias\n",
    "df.index = pd.date_range(start='2018-01-01 00:00:00',\n",
    "                         periods=len(df),\n",
    "                         freq = '5min')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
