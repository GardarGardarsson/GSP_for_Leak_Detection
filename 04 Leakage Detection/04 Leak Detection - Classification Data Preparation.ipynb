{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea40f6e",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "# Table of Contents:\n",
    "1. [Make Graph](#makegraph)\n",
    "2. [Read in Yearly Prediction and Scale Back to Original Interval](#readscale)\n",
    "3. [Exploratory Data Analysis](#eda)\n",
    "4. [1D CNN](#1dcnn)         <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88fb90b",
   "metadata": {},
   "source": [
    "# Leak Detection\n",
    "\n",
    "> Garðar Örn Garðarsson <br>\n",
    "Integrated Machine Learning Systems 20-21 <br>\n",
    "University College London"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d754a",
   "metadata": {},
   "source": [
    "<a id='makegraph'></a>\n",
    "*Back to [Table of Contents](#toc)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c9c5b",
   "metadata": {},
   "source": [
    "## 1. Make Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b838c",
   "metadata": {},
   "source": [
    "Convert the `EPANET` model to a `networkx` graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e944d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import epynet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.epanet_loader import get_nx_graph\n",
    "from utils.epanet_simulator import epanetSimulator\n",
    "from utils.data_loader import battledimLoader, dataCleaner, dataGenerator, embedSignalOnGraph, rescaleSignal\n",
    "from modules.torch_gnn import ChebNet\n",
    "from utils.visualisation import visualise\n",
    "\n",
    "# Runtime configuration\n",
    "path_to_wdn     = './data/L-TOWN.inp'\n",
    "path_to_data    = './data/l-town-data/'\n",
    "weight_mode     = 'pipe_length'\n",
    "self_loops      = True\n",
    "scaling         = 'minmax'\n",
    "figsize         = (50,16)\n",
    "print_out_rate  = 1               \n",
    "model_name      = 'l-town-chebnet-' + weight_mode +'-' + scaling + '{}'.format('-self_loop' if self_loops else '')\n",
    "last_model_path = './studies/models/' + model_name + '-1.pt'\n",
    "last_log_path   = './studies/logs/'   + model_name + '-1.csv' \n",
    "\n",
    "# Import the .inp file using the EPYNET library\n",
    "wdn = epynet.Network(path_to_wdn)\n",
    "\n",
    "# Solve hydraulic model for a single timestep\n",
    "wdn.solve()\n",
    "\n",
    "# Convert the file using a custom function, based on:\n",
    "# https://github.com/BME-SmartLab/GraphConvWat \n",
    "G , pos , head = get_nx_graph(wdn, weight_mode=weight_mode, get_head=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82305013",
   "metadata": {},
   "source": [
    "<a id='readscale'></a>\n",
    "*Back to [Table of Contents](#toc)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd48d98",
   "metadata": {},
   "source": [
    "## 2. Read in Yearly Prediction and Scale Back to Original Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04353b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prediction(filename='predictions.csv', scale=1, bias=0, start_date='2018-01-01 00:00:00'):\n",
    "    df = pd.read_csv(filename, index_col='Unnamed: 0')\n",
    "    df.columns = ['n{}'.format(int(node)+1) for node in df.columns]\n",
    "    df = df*scale+bias\n",
    "    df.index = pd.date_range(start=start_date,\n",
    "                             periods=len(df),\n",
    "                             freq = '5min')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a8607212",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 3                                              # Timesteps, t-1, t-2...t-n used to predict pressure at t\n",
    "sample_rate = 5                                              # Minutes sampling rate of data\n",
    "offset      = pd.DateOffset(minutes=sample_rate*n_timesteps) # We require n_timesteps of data our first prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344af04",
   "metadata": {},
   "source": [
    "Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6d91828c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p18 = read_prediction(filename='2019_predictions.csv',\n",
    "                      start_date=pd.Timestamp('2019-01-01 00:00:00')+offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af04436",
   "metadata": {},
   "source": [
    "Load reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a3443a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "r18 = read_prediction(filename='2019_reconstructions.csv',\n",
    "                      start_date='2019-01-01 00:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5e3742",
   "metadata": {},
   "source": [
    "Load leakage dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccb2045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l18 = pd.read_csv('data/l-town-data/2018_Leakages.csv',decimal=',',sep=';',index_col='Timestamp')\n",
    "l18.index = r18.index # Fix the index column timestamp format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4191cf1",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "*Back to [Table of Contents](#toc)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac1f7b",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40779cc6",
   "metadata": {},
   "source": [
    "### 3.1 Data Wrangling\n",
    "\n",
    "Lets create a dictionary of the format:\n",
    "\n",
    "`{ 'pipe_name' : [ connected_node_1 , connected_node_2 ] }`\n",
    "\n",
    "For all the pipes in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a839b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_by_pipe = {}\n",
    "\n",
    "for node in G:\n",
    "    for neighbour, connecting_edge in G[node].items():\n",
    "        if connecting_edge['name'] == 'SELF':\n",
    "            continue\n",
    "        else:\n",
    "            neighbours_by_pipe[connecting_edge['name']] = [node, neighbour]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013134b",
   "metadata": {},
   "source": [
    "Let's also create the inverse, when we want to look up pipes by their connecting nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf504f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_by_neighbours = { str(neighbour_list) : pipe for pipe , neighbour_list in neighbours_by_pipe.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eaf133",
   "metadata": {},
   "source": [
    "We'll also create a function to perform the lookup, as the order in which the nodes appear in the key matter and we don't bother with raising endless errors when looking up nodes we know to be connected, just cause we input them wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a79ccae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeByneighbourLookup(node1, node2, pipe_by_neighbours):\n",
    "    try:\n",
    "        return pipe_by_neighbours[str([node1,node2])]    # If we don't find the first combination\n",
    "    except:\n",
    "        try:                                            # We try the next\n",
    "            return pipe_by_neighbours[str([node2,node1])]\n",
    "        except:                                         # And if we still don't find it\n",
    "            return None                                 # We return nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a8f52d",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7947a546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p253'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeByneighbourLookup(1,347,pipe_by_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c323c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p253'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeByneighbourLookup(347,1,pipe_by_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70cd2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeByneighbourLookup(1,2,pipe_by_neighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d617fe",
   "metadata": {},
   "source": [
    "These might really come in handy when it comes to looking up pipe by node or for converting pipe leakage dataframes to leaky nodes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95156067",
   "metadata": {},
   "source": [
    "### 3.2 Calculating Per-Node Error\n",
    "\n",
    "We may calculate the node-wise validation error, $\\epsilon_{n_{i}}$, by subtracting the predicted values with the reconstructed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "42a9d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_node = (p18-r18).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f020b6",
   "metadata": {},
   "source": [
    "### 3.3 Calculating Per-Pipe Error\n",
    "\n",
    "We may calculate the pipe-wise validation errors, $\\epsilon_{p_i}$, as the difference of the validation error of the two nodes connecting the pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b17d3961",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error_by_pipe = {}\n",
    "\n",
    "for key,value in neighbours_by_pipe.items():\n",
    "    node_1 = 'n' + str(value[0])\n",
    "    node_2 = 'n' + str(value[1])\n",
    "    error_by_pipe[key] = ( error_by_node[node_1] - error_by_node[node_2] )\n",
    "    \n",
    "error_by_pipe = pd.DataFrame(error_by_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc7569c",
   "metadata": {},
   "source": [
    "### 3.4 Leakage Labelset\n",
    "\n",
    "Make a complete leakage labelset for each pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c5be6c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks_by_pipe = pd.DataFrame([], index=error_by_pipe.index, columns=error_by_pipe.columns)\n",
    "\n",
    "for leaky_pipe in l18:\n",
    "    leaks_by_pipe[leaky_pipe] = l18[leaky_pipe]\n",
    "    \n",
    "leaks_by_pipe = leaks_by_pipe.fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9881e03",
   "metadata": {},
   "source": [
    "Keep a dictionary of the timestamps of leakages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f78413ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_timestamps = {}\n",
    "\n",
    "for leak in l18:\n",
    "    leak_timestamps[leak] = l18.index[l18[leak]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efb0073f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['p31', 'p158', 'p183', 'p232', 'p257', 'p369', 'p427', 'p461', 'p538', 'p628', 'p654', 'p673', 'p810', 'p866'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leak_timestamps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72d32a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2018-06-29 01:35:00', '2018-06-29 01:40:00',\n",
       "               '2018-06-29 01:45:00'],\n",
       "              dtype='datetime64[ns]', freq='5T')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leak_timestamps['p31'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "792e45b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 40]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbours_by_pipe['p31']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b1a00",
   "metadata": {},
   "source": [
    "Alternatively, make a node-wise leak labelset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b68e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks_by_node = pd.DataFrame(data    = np.zeros(error_by_node.shape), \n",
    "                             index   = error_by_node.index, \n",
    "                             columns = np.arange(1,783))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbb2621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipe,neighbours in neighbours_by_pipe.items():\n",
    "    for neighbour in neighbours:\n",
    "        leaks_by_node[neighbour] += leaks_by_pipe[pipe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1554886f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-06-29 01:35:00     0.01\n",
       "2018-06-29 01:40:00     0.01\n",
       "2018-06-29 01:45:00     0.01\n",
       "2018-06-29 01:50:00     0.01\n",
       "2018-06-29 01:55:00     0.01\n",
       "                       ...  \n",
       "2018-08-12 17:10:00    16.01\n",
       "2018-08-12 17:15:00    16.01\n",
       "2018-08-12 17:20:00    16.00\n",
       "2018-08-12 17:25:00    15.99\n",
       "2018-08-12 17:30:00    16.00\n",
       "Freq: 5T, Name: 40, Length: 12864, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaks_by_node[40][leak_timestamps[pipeByneighbourLookup(40,42,pipe_by_neighbours)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "433b0610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>773</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:05:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:10:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:15:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:20:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:35:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:40:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:45:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:50:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:55:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105120 rows × 782 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1    2    3    4    5    6    7    8    9    10   ...  \\\n",
       "2018-01-01 00:00:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-01-01 00:05:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-01-01 00:10:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-01-01 00:15:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-01-01 00:20:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "...                  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "2018-12-31 23:35:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-12-31 23:40:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-12-31 23:45:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-12-31 23:50:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-12-31 23:55:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "                     773  774  775  776  777  778  779  780  781  782  \n",
       "2018-01-01 00:00:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-01-01 00:05:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-01-01 00:10:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-01-01 00:15:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-01-01 00:20:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...                  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "2018-12-31 23:35:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-12-31 23:40:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-12-31 23:45:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-12-31 23:50:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-12-31 23:55:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[105120 rows x 782 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaks_by_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd60dd90",
   "metadata": {},
   "source": [
    "### 3.5 Dataset Pre-Processing\n",
    "\n",
    "Process the dataset for timeseries classification. <br>\n",
    "I want to devise a set where I have a configurable window of observations, `[1, 5, 10, ... 200]`.<br>\n",
    "For that window of observations, I sum the per pipe events such that the according label will read `[0, 0, 1, 0, ...]`, where the set bit indicates the leaky pipes for that interval. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd1b708",
   "metadata": {},
   "source": [
    "First clean out the `NaNs` from the feature and labelset caused by the window size of the predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7f59e6c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_by_pipe = error_by_pipe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b94deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks_by_pipe = leaks_by_pipe[leaks_by_pipe.index.isin(error_by_pipe.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320368b4",
   "metadata": {},
   "source": [
    "Similarly for the pipe-wise classification, do it for the node-wise classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e01c8a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_by_node = error_by_node.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5419941",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks_by_node = leaks_by_node[leaks_by_node.index.isin(error_by_node.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab18c6",
   "metadata": {},
   "source": [
    "Split up the feature and labelset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "139c22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_pipe.to_csv('error_by_pipe_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6fddcd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks_by_pipe[leaks_by_pipe<4.5]=0\n",
    "leaks_by_pipe[leaks_by_pipe>0  ]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba3607e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks_by_pipe = leaks_by_pipe.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "59c07ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks_by_pipe.to_csv('leaks_by_pipe_r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92e92982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationTaskSplitter(x, y, using_window=True, window_size=10, data_slice=None):\n",
    "    \n",
    "    if not data_slice:\n",
    "        \n",
    "        data_slice = len(x)\n",
    "    \n",
    "    features = []\n",
    "    labels   = []\n",
    "    \n",
    "    if using_window:\n",
    "        \n",
    "        window_start = 0\n",
    "        window_end   = window_start+window_size\n",
    "\n",
    "        for i in range(len(x[window_start:window_start+data_slice-window_size])):\n",
    "            features.append( x.iloc[window_start:window_end].to_numpy() )\n",
    "            labels.append(  (y.iloc[window_start:window_end].sum().to_numpy() > 0).astype(int) )\n",
    "            window_start += 1\n",
    "            window_end   += 1\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for i in range(len(x[:data_slice-window_size])):\n",
    "            features.append( x.iloc[i].to_numpy() )\n",
    "            labels.append(  (y.iloc[i].to_numpy() > 0).astype(int) )\n",
    "    \n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef236998",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = classificationTaskSplitter(x            = error_by_pipe, \n",
    "                                  y            = leaks_by_pipe, \n",
    "                                  using_window = True,\n",
    "                                  window_size  = 24, \n",
    "                                  data_slice   = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c15416c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (105093, 24, 905) \n",
      "y: (105093, 905)\n"
     ]
    }
   ],
   "source": [
    "print(\"x: {} \\ny: {}\".format(x.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5c3ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "task   = 'pipe'\n",
    "window = 24\n",
    "name   = task + '_window_' + str(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d8d697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Volumes/GoogleDrive/Drifið mitt/00_leak_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2e2de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(name+'_x',x)\n",
    "np.save(name+'_y',y)\n",
    "#np.save(name+'idx',idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a213846",
   "metadata": {},
   "source": [
    "Create a random sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46d1ed",
   "metadata": {},
   "source": [
    "If we sample 10.000 pressure scenes (and their corresponding leak labels) from the 2018 data, that would amount to:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41c75477",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# 10.000 samples, 24 timestep, 782 pressure readings, 64 bit floating point each, 8 bits per byte, to giga\n",
    "100 * 24 * 782 * 64 / 8 / 1e9"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18bbbe7c",
   "metadata": {},
   "source": [
    "def classificationRandomSample(x, y, sample_size=10000, window_size=24, seed=None):\n",
    "    \n",
    "    # Draft a random sample of indices\n",
    "    np.random.seed(seed=1)\n",
    "    rand_idx = np.random.randint(low=0, high=len(x)-window_size, size=sample_size)\n",
    "    \n",
    "    # Initialise lists for the features and labels\n",
    "    features = []\n",
    "    labels   = []\n",
    "\n",
    "    for index in rand_idx:\n",
    "        features.append(x.iloc[index:index+window_size].to_numpy())\n",
    "        labels.append((y.iloc[index:index+window_size].sum().to_numpy() > 0).astype(int))\n",
    "    \n",
    "    features = np.array(features)\n",
    "    labels   = np.array(labels)\n",
    "    \n",
    "    return features, labels, rand_idx"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26351215",
   "metadata": {},
   "source": [
    "x,y,idx = classificationRandomSample(x           = error_by_pipe, \n",
    "                                     y           = leaks_by_pipe, \n",
    "                                     sample_size = 10000, \n",
    "                                     window_size = 6,\n",
    "                                     seed        = 18) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a129e0",
   "metadata": {},
   "source": [
    "<a id='1dcnn'></a>\n",
    "*Back to [Table of Contents](#toc)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1b50f",
   "metadata": {},
   "source": [
    "## 4. 1D-CNN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "713b2a71",
   "metadata": {},
   "source": [
    "np.save('x_wdw_6_18',x)\n",
    "np.save('y_wdw_6_18',y)\n",
    "np.save('idx_wdw_6_18',idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
