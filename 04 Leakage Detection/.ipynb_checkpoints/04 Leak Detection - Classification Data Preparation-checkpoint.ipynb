{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea40f6e",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "# Table of Contents:\n",
    "1. [Make Graph](#makegraph)\n",
    "2. [Read in Yearly Prediction and Scale Back to Original Interval](#readscale)\n",
    "3. [Exploratory Data Analysis](#eda)\n",
    "4. [1D CNN](#1dcnn)         <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88fb90b",
   "metadata": {},
   "source": [
    "# Leak Detection\n",
    "\n",
    "> Garðar Örn Garðarsson <br>\n",
    "Integrated Machine Learning Systems 20-21 <br>\n",
    "University College London"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d754a",
   "metadata": {},
   "source": [
    "<a id='makegraph'></a>\n",
    "*Back to [Table of Contents](#toc)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c9c5b",
   "metadata": {},
   "source": [
    "## 1. Make Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b838c",
   "metadata": {},
   "source": [
    "Convert the `EPANET` model to a `networkx` graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e944d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import epynet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.epanet_loader import get_nx_graph\n",
    "from utils.epanet_simulator import epanetSimulator\n",
    "from utils.data_loader import battledimLoader, dataCleaner, dataGenerator, embedSignalOnGraph, rescaleSignal\n",
    "from modules.torch_gnn import ChebNet\n",
    "from utils.visualisation import visualise\n",
    "\n",
    "# Runtime configuration\n",
    "path_to_wdn     = './data/L-TOWN.inp'\n",
    "path_to_data    = './data/l-town-data/'\n",
    "weight_mode     = 'pipe_length'\n",
    "self_loops      = True\n",
    "scaling         = 'minmax'\n",
    "figsize         = (50,16)\n",
    "print_out_rate  = 1               \n",
    "model_name      = 'l-town-chebnet-' + weight_mode +'-' + scaling + '{}'.format('-self_loop' if self_loops else '')\n",
    "last_model_path = './studies/models/' + model_name + '-1.pt'\n",
    "last_log_path   = './studies/logs/'   + model_name + '-1.csv' \n",
    "\n",
    "# Import the .inp file using the EPYNET library\n",
    "wdn = epynet.Network(path_to_wdn)\n",
    "\n",
    "# Solve hydraulic model for a single timestep\n",
    "wdn.solve()\n",
    "\n",
    "# Convert the file using a custom function, based on:\n",
    "# https://github.com/BME-SmartLab/GraphConvWat \n",
    "G , pos , head = get_nx_graph(wdn, weight_mode=weight_mode, get_head=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82305013",
   "metadata": {},
   "source": [
    "<a id='readscale'></a>\n",
    "*Back to [Table of Contents](#toc)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd48d98",
   "metadata": {},
   "source": [
    "## 2. Read in Yearly Prediction and Scale Back to Original Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04353b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prediction(filename='predictions.csv', scale=1, bias=0, start_date='2018-01-01 00:00:00'):\n",
    "    df = pd.read_csv(filename, index_col='Unnamed: 0')\n",
    "    df.columns = ['n{}'.format(int(node)+1) for node in df.columns]\n",
    "    df = df*scale+bias\n",
    "    df.index = pd.date_range(start=start_date,\n",
    "                             periods=len(df),\n",
    "                             freq = '5min')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8607212",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 3                                              # Timesteps, t-1, t-2...t-n used to predict pressure at t\n",
    "sample_rate = 5                                              # Minutes sampling rate of data\n",
    "offset      = pd.DateOffset(minutes=sample_rate*n_timesteps) # We require n_timesteps of data our first prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344af04",
   "metadata": {},
   "source": [
    "Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d91828c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p18 = read_prediction(filename='2018_predictions.csv',\n",
    "                      start_date=pd.Timestamp('2018-01-01 00:00:00')+offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af04436",
   "metadata": {},
   "source": [
    "Load reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3443a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "r18 = read_prediction(filename='2018_reconstructions.csv',\n",
    "                      start_date='2018-01-01 00:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5e3742",
   "metadata": {},
   "source": [
    "Load leakage dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccb2045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l18 = pd.read_csv('data/l-town-data/2018_Leakages.csv',decimal=',',sep=';',index_col='Timestamp')\n",
    "l18.index = r18.index # Fix the index column timestamp format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4191cf1",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "*Back to [Table of Contents](#toc)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac1f7b",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40779cc6",
   "metadata": {},
   "source": [
    "### 3.1 Data Wrangling\n",
    "\n",
    "Lets create a dictionary of the format:\n",
    "\n",
    "`{ 'pipe_name' : [ connected_node_1 , connected_node_2 ] }`\n",
    "\n",
    "For all the pipes in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a839b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_by_pipe = {}\n",
    "\n",
    "for node in G:\n",
    "    for neighbour, connecting_edge in G[node].items():\n",
    "        if connecting_edge['name'] == 'SELF':\n",
    "            continue\n",
    "        else:\n",
    "            neighbours_by_pipe[connecting_edge['name']] = [node, neighbour]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013134b",
   "metadata": {},
   "source": [
    "Let's also create the inverse, when we want to look up pipes by their connecting nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf504f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_by_neighbours = { str(neighbour_list) : pipe for pipe , neighbour_list in neighbours_by_pipe.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eaf133",
   "metadata": {},
   "source": [
    "We'll also create a function to perform the lookup, as the order in which the nodes appear in the key matter and we don't bother with raising endless errors when looking up nodes we know to be connected, just cause we input them wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a79ccae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeByneighbourLookup(node1, node2, pipe_by_neighbours):\n",
    "    try:\n",
    "        return pipe_by_neighbours[str([node1,node2])]    # If we don't find the first combination\n",
    "    except:\n",
    "        try:                                            # We try the next\n",
    "            return pipe_by_neighbours[str([node2,node1])]\n",
    "        except:                                         # And if we still don't find it\n",
    "            return None                                 # We return nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a8f52d",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7947a546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p253'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeByneighbourLookup(1,347,pipe_by_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c323c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p253'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeByneighbourLookup(347,1,pipe_by_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70cd2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeByneighbourLookup(1,2,pipe_by_neighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d617fe",
   "metadata": {},
   "source": [
    "These might really come in handy when it comes to looking up pipe by node or for converting pipe leakage dataframes to leaky nodes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95156067",
   "metadata": {},
   "source": [
    "### 3.2 Calculating Per-Node Error\n",
    "\n",
    "We may calculate the node-wise validation error, $\\epsilon_{n_{i}}$, by subtracting the predicted values with the reconstructed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42a9d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_node = (p18-r18).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f020b6",
   "metadata": {},
   "source": [
    "### 3.3 Calculating Per-Pipe Error\n",
    "\n",
    "We may calculate the pipe-wise validation errors, $\\epsilon_{p_i}$, as the difference of the validation error of the two nodes connecting the pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b17d3961",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error_by_pipe = {}\n",
    "\n",
    "for key,value in neighbours_by_pipe.items():\n",
    "    node_1 = 'n' + str(value[0])\n",
    "    node_2 = 'n' + str(value[1])\n",
    "    error_by_pipe[key] = ( error_by_node[node_1] - error_by_node[node_2] )\n",
    "    \n",
    "error_by_pipe = pd.DataFrame(error_by_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc7569c",
   "metadata": {},
   "source": [
    "### 3.4 Leakage Labelset\n",
    "\n",
    "Make a complete leakage labelset for each pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5be6c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks_by_pipe = pd.DataFrame([], index=error_by_pipe.index, columns=error_by_pipe.columns)\n",
    "\n",
    "for leaky_pipe in l18:\n",
    "    leaks_by_pipe[leaky_pipe] = l18[leaky_pipe]\n",
    "    \n",
    "leaks_by_pipe = leaks_by_pipe.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fb87eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p253</th>\n",
       "      <th>p5</th>\n",
       "      <th>p259</th>\n",
       "      <th>p261</th>\n",
       "      <th>p12</th>\n",
       "      <th>p241</th>\n",
       "      <th>p264</th>\n",
       "      <th>p7</th>\n",
       "      <th>p267</th>\n",
       "      <th>p11</th>\n",
       "      <th>...</th>\n",
       "      <th>p876</th>\n",
       "      <th>p879</th>\n",
       "      <th>p880</th>\n",
       "      <th>p881</th>\n",
       "      <th>p882</th>\n",
       "      <th>p883</th>\n",
       "      <th>p886</th>\n",
       "      <th>p887</th>\n",
       "      <th>p898</th>\n",
       "      <th>p901</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:05:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:10:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:15:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:20:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:35:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:40:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:45:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:50:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:55:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105120 rows × 905 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     p253   p5  p259  p261  p12  p241  p264   p7  p267  p11  \\\n",
       "2018-01-01 00:00:00   0.0  0.0   0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "2018-01-01 00:05:00   0.0  0.0   0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "2018-01-01 00:10:00   0.0  0.0   0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "2018-01-01 00:15:00   0.0  0.0   0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "2018-01-01 00:20:00   0.0  0.0   0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "...                   ...  ...   ...   ...  ...   ...   ...  ...   ...  ...   \n",
       "2018-12-31 23:35:00   0.0  0.0   0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "2018-12-31 23:40:00   0.0  0.0   0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "2018-12-31 23:45:00   0.0  0.0   0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "2018-12-31 23:50:00   0.0  0.0   0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "2018-12-31 23:55:00   0.0  0.0   0.0   0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "\n",
       "                     ...  p876  p879  p880  p881  p882  p883  p886  p887  \\\n",
       "2018-01-01 00:00:00  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2018-01-01 00:05:00  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2018-01-01 00:10:00  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2018-01-01 00:15:00  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2018-01-01 00:20:00  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "...                  ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "2018-12-31 23:35:00  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2018-12-31 23:40:00  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2018-12-31 23:45:00  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2018-12-31 23:50:00  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2018-12-31 23:55:00  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "                     p898  p901  \n",
       "2018-01-01 00:00:00   0.0   0.0  \n",
       "2018-01-01 00:05:00   0.0   0.0  \n",
       "2018-01-01 00:10:00   0.0   0.0  \n",
       "2018-01-01 00:15:00   0.0   0.0  \n",
       "2018-01-01 00:20:00   0.0   0.0  \n",
       "...                   ...   ...  \n",
       "2018-12-31 23:35:00   0.0   0.0  \n",
       "2018-12-31 23:40:00   0.0   0.0  \n",
       "2018-12-31 23:45:00   0.0   0.0  \n",
       "2018-12-31 23:50:00   0.0   0.0  \n",
       "2018-12-31 23:55:00   0.0   0.0  \n",
       "\n",
       "[105120 rows x 905 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaks_by_pipe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9881e03",
   "metadata": {},
   "source": [
    "Keep a dictionary of the timestamps of leakages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f78413ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_timestamps = {}\n",
    "\n",
    "for leak in l18:\n",
    "    leak_timestamps[leak] = l18.index[l18[leak]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efb0073f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['p31', 'p158', 'p183', 'p232', 'p257', 'p369', 'p427', 'p461', 'p538', 'p628', 'p654', 'p673', 'p810', 'p866'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leak_timestamps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72d32a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2018-06-29 01:35:00', '2018-06-29 01:40:00',\n",
       "               '2018-06-29 01:45:00'],\n",
       "              dtype='datetime64[ns]', freq='5T')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leak_timestamps['p31'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "792e45b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 40]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbours_by_pipe['p31']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b1a00",
   "metadata": {},
   "source": [
    "Alternatively, make a node-wise leak labelset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b68e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks_by_node = pd.DataFrame(data    = np.zeros(error_by_node.shape), \n",
    "                             index   = error_by_node.index, \n",
    "                             columns = np.arange(1,783))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbb2621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipe,neighbours in neighbours_by_pipe.items():\n",
    "    for neighbour in neighbours:\n",
    "        leaks_by_node[neighbour] += leaks_by_pipe[pipe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1554886f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-06-29 01:35:00     0.01\n",
       "2018-06-29 01:40:00     0.01\n",
       "2018-06-29 01:45:00     0.01\n",
       "2018-06-29 01:50:00     0.01\n",
       "2018-06-29 01:55:00     0.01\n",
       "                       ...  \n",
       "2018-08-12 17:10:00    16.01\n",
       "2018-08-12 17:15:00    16.01\n",
       "2018-08-12 17:20:00    16.00\n",
       "2018-08-12 17:25:00    15.99\n",
       "2018-08-12 17:30:00    16.00\n",
       "Freq: 5T, Name: 40, Length: 12864, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaks_by_node[40][leak_timestamps[pipeByneighbourLookup(40,42,pipe_by_neighbours)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "433b0610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>773</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:05:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:10:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:15:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:20:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:35:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:40:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:45:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:50:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:55:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105120 rows × 782 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1    2    3    4    5    6    7    8    9    10   ...  \\\n",
       "2018-01-01 00:00:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-01-01 00:05:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-01-01 00:10:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-01-01 00:15:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-01-01 00:20:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "...                  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "2018-12-31 23:35:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-12-31 23:40:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-12-31 23:45:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-12-31 23:50:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2018-12-31 23:55:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "                     773  774  775  776  777  778  779  780  781  782  \n",
       "2018-01-01 00:00:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-01-01 00:05:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-01-01 00:10:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-01-01 00:15:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-01-01 00:20:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...                  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "2018-12-31 23:35:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-12-31 23:40:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-12-31 23:45:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-12-31 23:50:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2018-12-31 23:55:00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[105120 rows x 782 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaks_by_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd60dd90",
   "metadata": {},
   "source": [
    "### 3.5 Dataset Pre-Processing\n",
    "\n",
    "Process the dataset for timeseries classification. <br>\n",
    "I want to devise a set where I have a configurable window of observations, `[1, 5, 10, ... 200]`.<br>\n",
    "For that window of observations, I sum the per pipe events such that the according label will read `[0, 0, 1, 0, ...]`, where the set bit indicates the leaky pipes for that interval. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd1b708",
   "metadata": {},
   "source": [
    "First clean out the `NaNs` from the feature and labelset caused by the window size of the predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f59e6c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_by_pipe = error_by_pipe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b94deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks_by_pipe = leaks_by_pipe[leaks_by_pipe.index.isin(error_by_pipe.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320368b4",
   "metadata": {},
   "source": [
    "Similarly for the pipe-wise classification, do it for the node-wise classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e01c8a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_by_node = error_by_node.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5419941",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks_by_node = leaks_by_node[leaks_by_node.index.isin(error_by_node.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab18c6",
   "metadata": {},
   "source": [
    "Split up the feature and labelset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92e92982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationTaskSplitter(x, y, using_window=True, window_size=10, data_slice=None):\n",
    "    \n",
    "    if not data_slice:\n",
    "        \n",
    "        data_slice = len(x)\n",
    "    \n",
    "    features = []\n",
    "    labels   = []\n",
    "    \n",
    "    if using_window:\n",
    "        \n",
    "        window_start = 0\n",
    "        window_end   = window_start+window_size\n",
    "\n",
    "        for i in range(len(x[window_start:window_start+data_slice-window_size])):\n",
    "            features.append( x.iloc[window_start:window_end].to_numpy() )\n",
    "            labels.append(  (y.iloc[window_start:window_end].sum().to_numpy() > 0).astype(int) )\n",
    "            window_start += 1\n",
    "            window_end   += 1\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for i in range(len(x[:data_slice-window_size])):\n",
    "            features.append( x.iloc[i].to_numpy() )\n",
    "            labels.append(  (y.iloc[i].to_numpy() > 0).astype(int) )\n",
    "    \n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef236998",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = classificationTaskSplitter(x            = error_by_pipe, \n",
    "                                  y            = leaks_by_pipe, \n",
    "                                  using_window = True,\n",
    "                                  window_size  = 24, \n",
    "                                  data_slice   = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c15416c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (105093, 24, 905) \n",
      "y: (105093, 905)\n"
     ]
    }
   ],
   "source": [
    "print(\"x: {} \\ny: {}\".format(x.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5c3ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "task   = 'pipe'\n",
    "window = 24\n",
    "name   = task + '_window_' + str(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d8d697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Volumes/GoogleDrive/Drifið mitt/00_leak_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2e2de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(name+'_x',x)\n",
    "np.save(name+'_y',y)\n",
    "#np.save(name+'idx',idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a213846",
   "metadata": {},
   "source": [
    "Create a random sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46d1ed",
   "metadata": {},
   "source": [
    "If we sample 10.000 pressure scenes (and their corresponding leak labels) from the 2018 data, that would amount to:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41c75477",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# 10.000 samples, 24 timestep, 782 pressure readings, 64 bit floating point each, 8 bits per byte, to giga\n",
    "100 * 24 * 782 * 64 / 8 / 1e9"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18bbbe7c",
   "metadata": {},
   "source": [
    "def classificationRandomSample(x, y, sample_size=10000, window_size=24, seed=None):\n",
    "    \n",
    "    # Draft a random sample of indices\n",
    "    np.random.seed(seed=1)\n",
    "    rand_idx = np.random.randint(low=0, high=len(x)-window_size, size=sample_size)\n",
    "    \n",
    "    # Initialise lists for the features and labels\n",
    "    features = []\n",
    "    labels   = []\n",
    "\n",
    "    for index in rand_idx:\n",
    "        features.append(x.iloc[index:index+window_size].to_numpy())\n",
    "        labels.append((y.iloc[index:index+window_size].sum().to_numpy() > 0).astype(int))\n",
    "    \n",
    "    features = np.array(features)\n",
    "    labels   = np.array(labels)\n",
    "    \n",
    "    return features, labels, rand_idx"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26351215",
   "metadata": {},
   "source": [
    "x,y,idx = classificationRandomSample(x           = error_by_pipe, \n",
    "                                     y           = leaks_by_pipe, \n",
    "                                     sample_size = 10000, \n",
    "                                     window_size = 6,\n",
    "                                     seed        = 18) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a129e0",
   "metadata": {},
   "source": [
    "<a id='1dcnn'></a>\n",
    "*Back to [Table of Contents](#toc)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1b50f",
   "metadata": {},
   "source": [
    "## 4. 1D-CNN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "713b2a71",
   "metadata": {},
   "source": [
    "np.save('x_wdw_6_18',x)\n",
    "np.save('y_wdw_6_18',y)\n",
    "np.save('idx_wdw_6_18',idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
