{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88fb90b",
   "metadata": {},
   "source": [
    "# Leak Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c9c5b",
   "metadata": {},
   "source": [
    "## 1. Make Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b838c",
   "metadata": {},
   "source": [
    "Convert the `EPANET` model to a `networkx` graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e944d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import epynet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.epanet_loader import get_nx_graph\n",
    "from utils.epanet_simulator import epanetSimulator\n",
    "from utils.data_loader import battledimLoader, dataCleaner, dataGenerator, embedSignalOnGraph, rescaleSignal\n",
    "from modules.torch_gnn import ChebNet\n",
    "from utils.visualisation import visualise\n",
    "\n",
    "# Runtime configuration\n",
    "path_to_wdn     = './data/L-TOWN.inp'\n",
    "path_to_data    = './data/l-town-data/'\n",
    "weight_mode     = 'pipe_length'\n",
    "self_loops      = True\n",
    "scaling         = 'minmax'\n",
    "figsize         = (50,16)\n",
    "print_out_rate  = 1               \n",
    "model_name      = 'l-town-chebnet-' + weight_mode +'-' + scaling + '{}'.format('-self_loop' if self_loops else '')\n",
    "last_model_path = './studies/models/' + model_name + '-1.pt'\n",
    "last_log_path   = './studies/logs/'   + model_name + '-1.csv' \n",
    "\n",
    "# Import the .inp file using the EPYNET library\n",
    "wdn = epynet.Network(path_to_wdn)\n",
    "\n",
    "# Solve hydraulic model for a single timestep\n",
    "wdn.solve()\n",
    "\n",
    "# Convert the file using a custom function, based on:\n",
    "# https://github.com/BME-SmartLab/GraphConvWat \n",
    "G , pos , head = get_nx_graph(wdn, weight_mode=weight_mode, get_head=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813e8928",
   "metadata": {},
   "source": [
    "Get a list of node IDs with sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddec7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset configuration file\n",
    "with open(path_to_data + 'dataset_configuration.yml') as file:\n",
    "\n",
    "    # Load the configuration to a dictionary\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader) \n",
    "\n",
    "# Generate a list of integers, indicating the number of the node\n",
    "# at which a  pressure sensor is present\n",
    "sensors = [int(string.replace(\"n\", \"\")) for string in config['pressure_sensors']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39199da",
   "metadata": {},
   "source": [
    "Add self-loops to the sensor nodes in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3ceb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if self_loops:\n",
    "    for sensor_node in sensors:             # For each node in the sensor list\n",
    "        G.add_edge(u_of_edge=sensor_node,   # Add an edge from that node ...\n",
    "                   v_of_edge=sensor_node,   # ... to itself ...\n",
    "                   weight=1.,name='SELF')   # ... and set its weight to equal 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d40ab9",
   "metadata": {},
   "source": [
    "## 2. Get Simulation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1de321",
   "metadata": {},
   "source": [
    "We run an EPANET simulation using the WNTR library and the EPANET\n",
    "nominal model supplied with the BattLeDIM competition. <br>\n",
    "With this simulation, we have a complete pressure signal for all\n",
    "nodes in the network, on which the GNN algorithm is to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd89db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the nominal WDN model\n",
    "nominal_wdn_model = epanetSimulator(path_to_wdn, path_to_data)\n",
    "\n",
    "# Run a simulation\n",
    "nominal_wdn_model.simulate()\n",
    "\n",
    "# Retrieve the nodal pressures\n",
    "nominal_pressure = nominal_wdn_model.get_simulated_pressure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230405b",
   "metadata": {},
   "source": [
    "Populate feature vector x and label vector y from the nominal pressures. <br>\n",
    "Also retrieve the scale and bias of the scaling transformation. <br>\n",
    "This is so we can inverse transform the predicted values to calculate relative reconstruction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5376c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,scale,bias = dataCleaner(pressure_df    = nominal_pressure, # Pass the nodal pressures\n",
    "                             observed_nodes = sensors,          # Indicate which nodes have sensors\n",
    "                             rescale        = scaling)          # Perform scaling on the timeseries data\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, \n",
    "                                              test_size    = 0.2,\n",
    "                                              random_state = 1,\n",
    "                                              shuffle      = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88350332",
   "metadata": {},
   "source": [
    "## 3. Get Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf14ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a numpy array with format matching the GraphConvWat problem\n",
    "pressure_2018 = battledimLoader(observed_nodes = sensors,\n",
    "                                n_nodes        = 782,\n",
    "                                path           = path_to_data,\n",
    "                                file           = '2018_SCADA_Pressures.csv',\n",
    "                                rescale        = True, \n",
    "                                scale          = scale,\n",
    "                                bias           = bias)\n",
    "\n",
    "pressure_2019 = battledimLoader(observed_nodes = sensors,\n",
    "                                n_nodes        = 782,\n",
    "                                path           = path_to_data,\n",
    "                                file           = '2019_SCADA_Pressures.csv',\n",
    "                                rescale        = True, \n",
    "                                scale          = scale,\n",
    "                                bias           = bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820a46a",
   "metadata": {},
   "source": [
    "## 4. Load a Trained GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7784fe40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the computation device as NVIDIA GPU if available else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate a Chebysev Network GNN model\n",
    "model  = ChebNet(name           = 'ChebNet',\n",
    "                 data_generator = None,\n",
    "                 device         = device, \n",
    "                 in_channels    = np.shape(x_trn)[-1], \n",
    "                 out_channels   = np.shape(y_trn)[-1],\n",
    "                 data_scale     = scale, \n",
    "                 data_bias      = bias).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd49ed5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Loaded previous model results...\n",
      "                --------------------------------------------------\n",
      "                Model has been trained for:\t100 epochs\n",
      "                Best validation loss:      \t1.6016847697140293e-05 \n",
      "                Occurred in training round:\t96 \n"
     ]
    }
   ],
   "source": [
    "# We offer the user the option to load the previously trained weights\n",
    "model.load_model(last_model_path, last_log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce5b2b",
   "metadata": {},
   "source": [
    "## 5. Predict a Year's worth of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b683449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pressure(graph, pressure_series, print_out_rate=100, save=True, filename='predictions.csv'):\n",
    "    results = []\n",
    "    elapsed_time = time.time()\n",
    "    for i, partial_graph_signal in enumerate(pressure_series):\n",
    "        if not i % print_out_rate:\n",
    "            execution_time = time.time()\n",
    "        \n",
    "        results.append(model.predict(graph, partial_graph_signal))\n",
    "        \n",
    "        if not i % print_out_rate:\n",
    "            print('Signal:\\t{}\\t Execution:\\t{:.3f} s\\t Elapsed:\\t{:.3f} s'.format(i,\n",
    "                                                                                   time.time()-execution_time, \n",
    "                                                                                   time.time()-elapsed_time))\n",
    "    if save:\n",
    "        print('-'*63+\"\\nSaving results to: \\n{}/{}\\n\\n\".format(os.getcwd(),filename))\n",
    "        pd.DataFrame(results).to_csv(filename)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "018a9b97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal:\t0\t Execution:\t0.257 s\t Elapsed:\t0.257 s\n",
      "Signal:\t1\t Execution:\t0.250 s\t Elapsed:\t0.507 s\n",
      "Signal:\t2\t Execution:\t0.240 s\t Elapsed:\t0.747 s\n",
      "Signal:\t3\t Execution:\t0.248 s\t Elapsed:\t0.995 s\n",
      "Signal:\t4\t Execution:\t0.253 s\t Elapsed:\t1.249 s\n",
      "Signal:\t5\t Execution:\t0.243 s\t Elapsed:\t1.491 s\n",
      "Signal:\t6\t Execution:\t0.250 s\t Elapsed:\t1.741 s\n",
      "Signal:\t7\t Execution:\t0.252 s\t Elapsed:\t1.993 s\n",
      "Signal:\t8\t Execution:\t0.247 s\t Elapsed:\t2.240 s\n",
      "Signal:\t9\t Execution:\t0.243 s\t Elapsed:\t2.484 s\n",
      "Signal:\t10\t Execution:\t0.245 s\t Elapsed:\t2.729 s\n",
      "---------------------------------------------------------------\n",
      "Saving results to: \n",
      "/Users/gardar/Documents/UCL/ELEC0054 IMLS Research Project/04 Implementation/04 Leakage Detection/2018_predictions.csv\n",
      "\n",
      "\n",
      "Signal:\t0\t Execution:\t0.260 s\t Elapsed:\t0.260 s\n",
      "Signal:\t1\t Execution:\t0.262 s\t Elapsed:\t0.522 s\n",
      "Signal:\t2\t Execution:\t0.253 s\t Elapsed:\t0.775 s\n",
      "Signal:\t3\t Execution:\t0.274 s\t Elapsed:\t1.050 s\n",
      "Signal:\t4\t Execution:\t0.266 s\t Elapsed:\t1.316 s\n",
      "Signal:\t5\t Execution:\t0.284 s\t Elapsed:\t1.600 s\n",
      "Signal:\t6\t Execution:\t0.261 s\t Elapsed:\t1.861 s\n",
      "Signal:\t7\t Execution:\t0.243 s\t Elapsed:\t2.105 s\n",
      "Signal:\t8\t Execution:\t0.241 s\t Elapsed:\t2.346 s\n",
      "Signal:\t9\t Execution:\t0.277 s\t Elapsed:\t2.623 s\n",
      "Signal:\t10\t Execution:\t0.275 s\t Elapsed:\t2.898 s\n",
      "---------------------------------------------------------------\n",
      "Saving results to: \n",
      "/Users/gardar/Documents/UCL/ELEC0054 IMLS Research Project/04 Implementation/04 Leakage Detection/2019_predictions.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_2018 = predict_pressure(G, \n",
    "                                   pressure_2018[:11], \n",
    "                                   print_out_rate = print_out_rate, \n",
    "                                   save           = True, \n",
    "                                   filename       = '2018_predictions.csv')\n",
    "\n",
    "prediction_2019 = predict_pressure(G, \n",
    "                                   pressure_2019[:11], \n",
    "                                   print_out_rate = print_out_rate, \n",
    "                                   save           = True, \n",
    "                                   filename       = '2019_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd48d98",
   "metadata": {},
   "source": [
    "## 6. Read in Yearly Prediction and Scale Back to Original Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ae7bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2018_predictions.csv', index_col='Unnamed: 0')\n",
    "df.columns = ['n{}'.format(int(node)+1) for node in df.columns]\n",
    "df = df*scale+bias\n",
    "df.index = pd.date_range(start='2018-01-01 00:00:00',\n",
    "                         periods=len(df),\n",
    "                         freq = '5min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5188f422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "24f67ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prediction(filename='predictions.csv', scale=1, bias=0, start_date='2018-01-01 00:00:00'):\n",
    "    df = pd.read_csv(filename, index_col='Unnamed: 0')\n",
    "    df.columns = ['n{}'.format(int(node)+1) for node in df.columns]\n",
    "    df = df*scale+bias\n",
    "    df.index = pd.date_range(start='2018-01-01 00:00:00',\n",
    "                             periods=len(df),\n",
    "                             freq = '5min')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "df74a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "p18 = read_prediction(filename='2018_predictions.csv',\n",
    "                      scale=scale,\n",
    "                      bias=bias,\n",
    "                      start_date='2018-01-01 00:00:00')\n",
    "p19 = read_prediction(filename='2019_predictions.csv',\n",
    "                      scale=scale,\n",
    "                      bias=bias,\n",
    "                      start_date='2019-01-01 00:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5b7d5678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-01-01 00:00:00    0.054518\n",
       "2018-01-01 00:05:00    0.053759\n",
       "2018-01-01 00:10:00    0.055393\n",
       "2018-01-01 00:15:00    0.055423\n",
       "2018-01-01 00:20:00    0.057122\n",
       "2018-01-01 00:25:00    0.056516\n",
       "2018-01-01 00:30:00    0.056592\n",
       "2018-01-01 00:35:00    0.058773\n",
       "2018-01-01 00:40:00    0.058305\n",
       "2018-01-01 00:45:00    0.059527\n",
       "2018-01-01 00:50:00    0.057586\n",
       "Freq: 5T, Name: n1, dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p18-p19)['n1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ba899c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n99'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(abs(p18-p19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cfd6a910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-01-01 00:00:00    0.174519\n",
       "2018-01-01 00:05:00    0.160510\n",
       "2018-01-01 00:10:00    0.161111\n",
       "2018-01-01 00:15:00    0.156778\n",
       "2018-01-01 00:20:00    0.170778\n",
       "2018-01-01 00:25:00    0.161351\n",
       "2018-01-01 00:30:00    0.151379\n",
       "2018-01-01 00:35:00    0.157338\n",
       "2018-01-01 00:40:00    0.154290\n",
       "2018-01-01 00:45:00    0.152818\n",
       "2018-01-01 00:50:00    0.146266\n",
       "Freq: 5T, Name: n99, dtype: float64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p18-p19)['n99']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "88a40262",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (p18-p19).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "eb59a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = diff.rank(axis=1,method='max',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "246df23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n273      1.0\n",
       "n235      2.0\n",
       "n129      3.0\n",
       "n144      4.0\n",
       "n341      5.0\n",
       "        ...  \n",
       "n359    778.0\n",
       "n358    779.0\n",
       "n22     780.0\n",
       "n7      781.0\n",
       "n336    782.0\n",
       "Name: 2018-01-01 00:00:00, Length: 782, dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank.iloc[0].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "63e31ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-01-01 00:00:00    0.232721\n",
       "2018-01-01 00:05:00    0.215916\n",
       "2018-01-01 00:10:00    0.216836\n",
       "2018-01-01 00:15:00    0.211407\n",
       "2018-01-01 00:20:00    0.229901\n",
       "2018-01-01 00:25:00    0.215514\n",
       "2018-01-01 00:30:00    0.201529\n",
       "2018-01-01 00:35:00    0.211692\n",
       "2018-01-01 00:40:00    0.206491\n",
       "2018-01-01 00:45:00    0.205480\n",
       "2018-01-01 00:50:00    0.195376\n",
       "Freq: 5T, Name: n273, dtype: float64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff['n273']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1d551562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-01-01 00:00:00    0.018312\n",
       "2018-01-01 00:05:00    0.016530\n",
       "2018-01-01 00:10:00    0.016677\n",
       "2018-01-01 00:15:00    0.016357\n",
       "2018-01-01 00:20:00    0.017919\n",
       "2018-01-01 00:25:00    0.016823\n",
       "2018-01-01 00:30:00    0.015407\n",
       "2018-01-01 00:35:00    0.016401\n",
       "2018-01-01 00:40:00    0.015862\n",
       "2018-01-01 00:45:00    0.015856\n",
       "2018-01-01 00:50:00    0.015079\n",
       "Freq: 5T, Name: n336, dtype: float64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff['n336']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1bd70a",
   "metadata": {},
   "source": [
    "Animate graph of network (?)\n",
    "\n",
    "Create an animation\n",
    "\n",
    "Standardise pressure difference rankings (divide by node number)\n",
    "\n",
    "Plot, then biggest difference should be \"whitest\" on colormap, to darkest where diff is little."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
